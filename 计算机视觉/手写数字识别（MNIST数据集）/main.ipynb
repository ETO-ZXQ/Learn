{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-04-10T10:46:05.017960Z",
     "iopub.status.busy": "2022-04-10T10:46:05.017183Z",
     "iopub.status.idle": "2022-04-10T10:46:05.822090Z",
     "shell.execute_reply": "2022-04-10T10:46:05.820841Z",
     "shell.execute_reply.started": "2022-04-10T10:46:05.017895Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data65\n"
     ]
    }
   ],
   "source": [
    "# 查看当前挂载的数据集目录, 该目录下的变更重启环境后会自动还原\n",
    "# View dataset directory. \n",
    "# This directory will be recovered automatically after resetting environment. \n",
    "!ls /home/aistudio/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-04-10T10:46:05.824732Z",
     "iopub.status.busy": "2022-04-10T10:46:05.824318Z",
     "iopub.status.idle": "2022-04-10T10:46:06.152082Z",
     "shell.execute_reply": "2022-04-10T10:46:06.150813Z",
     "shell.execute_reply.started": "2022-04-10T10:46:05.824691Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 查看工作区文件, 该目录下的变更将会持久保存. 请及时清理不必要的文件, 避免加载过慢.\n",
    "# View personal work directory. \n",
    "# All changes under this directory will be kept even after reset. \n",
    "# Please clean unnecessary files in time to speed up environment loading. \n",
    "!ls /home/aistudio/work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 手写数字识别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-10T10:46:06.154328Z",
     "iopub.status.busy": "2022-04-10T10:46:06.153993Z",
     "iopub.status.idle": "2022-04-10T10:46:06.160090Z",
     "shell.execute_reply": "2022-04-10T10:46:06.159222Z",
     "shell.execute_reply.started": "2022-04-10T10:46:06.154289Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import paddle\n",
    "import numpy as np\n",
    "from paddle.nn import Conv2D, MaxPool2D, Linear\n",
    "import paddle.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义卷积神经网络LeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-10T10:46:06.163086Z",
     "iopub.status.busy": "2022-04-10T10:46:06.162462Z",
     "iopub.status.idle": "2022-04-10T10:46:06.177597Z",
     "shell.execute_reply": "2022-04-10T10:46:06.176529Z",
     "shell.execute_reply.started": "2022-04-10T10:46:06.163041Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LeNet(paddle.nn.Layer):\n",
    "    def __init__(self, num_classes=2):  # num_classes为分类类数\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = Conv2D(in_channels=1, out_channels=6, kernel_size=5) \n",
    "        self.max_pool1 = MaxPool2D(kernel_size=2, stride=2)\n",
    "        self.conv2 = Conv2D(in_channels=6, out_channels=16, kernel_size=5)\n",
    "        self.max_pool2 = MaxPool2D(kernel_size=2, stride=2)\n",
    "        self.conv3 = Conv2D(in_channels=16, out_channels=120, kernel_size=4)\n",
    "        self.fc1 = Linear(in_features=120, out_features=64)\n",
    "        self.fc2 = Linear(in_features=64, out_features=num_classes)\n",
    "    \n",
    "    def forward(self, x):                   #[ N ,1,28,28]\n",
    "        x = self.conv1(x)                   #[ N ,6,24,24]\n",
    "        x = F.sigmoid(x)                    #[ N ,6,24,24]\n",
    "        x = self.max_pool1(x)               #[ N ,6,12,12]\n",
    "        x = F.sigmoid(x)                    #[ N ,6,12,12]\n",
    "        x = self.conv2(x)                   #[ N ,16,8,8]\n",
    "        x = self.max_pool2(x)               #[ N ,16,4,4]\n",
    "        x = self.conv3(x)                   #[ N ,120,1,1]\n",
    "        x = paddle.reshape(x, [x.shape[0], -1])   #[ N ,120]\n",
    "        x = self.fc1(x)                     #[ N ,64]\n",
    "        x = F.sigmoid(x)                    #[ N ,64]\n",
    "        x = self.fc2(x)                     #[ N ,10]\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-10T10:46:06.179224Z",
     "iopub.status.busy": "2022-04-10T10:46:06.178974Z",
     "iopub.status.idle": "2022-04-10T10:46:06.192624Z",
     "shell.execute_reply": "2022-04-10T10:46:06.191616Z",
     "shell.execute_reply.started": "2022-04-10T10:46:06.179195Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import imp\n",
    "from paddle.vision.transforms import ToTensor\n",
    "from paddle.vision.datasets import MNIST\n",
    "\n",
    "\n",
    "def train(model, opt, train_loader, valid_loader):\n",
    "    use_gpu = True\n",
    "    #paddle.device.set_device('gpu:0') if use_gpu else paddle.device.set_device('cpu')\n",
    "    print('-----------start training----------')\n",
    "\n",
    "    # 训练模式\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(EPOCH_NUM):\n",
    "        for batch_id, data in enumerate(train_loader()):\n",
    "            img = data[0]              #[10,1,28,28]\n",
    "            label = data[1]            #[10,1]\n",
    "            # 计算模型输出\n",
    "            logits = model(img)\n",
    "            # 计算损失函数\n",
    "            loss_func = paddle.nn.CrossEntropyLoss(reduction='none')\n",
    "            loss = loss_func(logits, label)\n",
    "            avg_loss = paddle.mean(loss)\n",
    "\n",
    "\n",
    "            if batch_id % 500 == 0:\n",
    "                print(\"epoch: {}, batch_id: {:4}, loss is: {:.4f}\".format(epoch+1, batch_id, float(avg_loss.numpy())))\n",
    "            avg_loss.backward()\n",
    "            opt.step()\n",
    "            opt.clear_grad()\n",
    "\n",
    "        # 切换评价模式\n",
    "        model.eval()\n",
    "\n",
    "        accuracies = []\n",
    "        losses = []\n",
    "        for batch_id, data in enumerate(valid_loader()):\n",
    "            img = data[0]\n",
    "            label = data[1] \n",
    "            # 计算模型输出\n",
    "            logits = model(img)\n",
    "            # 计算损失函数\n",
    "            loss_func = paddle.nn.CrossEntropyLoss(reduction='none')\n",
    "            loss = loss_func(logits, label)\n",
    "            acc = paddle.metric.accuracy(logits, label)\n",
    "            accuracies.append(acc.numpy())\n",
    "            losses.append(loss.numpy())\n",
    "\n",
    "        global all_train_iter  \n",
    "        all_train_iter = all_train_iter + 1\n",
    "        all_train_iters.append(all_train_iter)\n",
    "        all_train_costs.append(np.mean(losses))\n",
    "        all_train_accs.append(np.mean(accuracies))\n",
    "\n",
    "        \n",
    "        print(\"[validation] accuracy/loss: {:.4f}/{:.4f}\".format(np.mean(accuracies), np.mean(losses)))\n",
    "        \n",
    "        model.train()    # 再次切换到训练模式\n",
    "\n",
    "    # 保存模型参数\n",
    "    paddle.save(model.state_dict(), 'mnist_num_OCR.pdparams')\n",
    "\n",
    "\n",
    "    print('-----------finish training----------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义绘制模型训练曲线函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-10T10:46:06.194274Z",
     "iopub.status.busy": "2022-04-10T10:46:06.194009Z",
     "iopub.status.idle": "2022-04-10T10:46:06.200450Z",
     "shell.execute_reply": "2022-04-10T10:46:06.199749Z",
     "shell.execute_reply.started": "2022-04-10T10:46:06.194244Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def draw_train_process(title,iters,costs,accs,label_cost,lable_acc):\r\n",
    "    plt.title(title, fontsize=24)\r\n",
    "    plt.xlabel(\"iter\", fontsize=20)\r\n",
    "    plt.ylabel(\"cost/acc\", fontsize=20)\r\n",
    "    plt.plot(iters, costs,color='red',label=label_cost) \r\n",
    "    plt.plot(iters, accs,color='green',label=lable_acc) \r\n",
    "    plt.legend()\r\n",
    "    plt.grid()\r\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 主函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-10T10:46:06.202263Z",
     "iopub.status.busy": "2022-04-10T10:46:06.201580Z",
     "iopub.status.idle": "2022-04-10T12:20:48.814404Z",
     "shell.execute_reply": "2022-04-10T12:20:48.813589Z",
     "shell.execute_reply.started": "2022-04-10T10:46:06.202225Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------start training----------\n",
      "epoch: 1, batch_id:    0, loss is: 2.4507\n",
      "epoch: 1, batch_id:  500, loss is: 2.3270\n",
      "epoch: 1, batch_id: 1000, loss is: 2.2813\n",
      "epoch: 1, batch_id: 1500, loss is: 2.2350\n",
      "epoch: 1, batch_id: 2000, loss is: 2.2141\n",
      "epoch: 1, batch_id: 2500, loss is: 2.3076\n",
      "epoch: 1, batch_id: 3000, loss is: 2.1791\n",
      "epoch: 1, batch_id: 3500, loss is: 2.0481\n",
      "epoch: 1, batch_id: 4000, loss is: 1.4203\n",
      "epoch: 1, batch_id: 4500, loss is: 1.1195\n",
      "epoch: 1, batch_id: 5000, loss is: 0.9389\n",
      "epoch: 1, batch_id: 5500, loss is: 0.4750\n",
      "[validation] accuracy/loss: 0.8166/0.6720\n",
      "epoch: 2, batch_id:    0, loss is: 0.9963\n",
      "epoch: 2, batch_id:  500, loss is: 0.5001\n",
      "epoch: 2, batch_id: 1000, loss is: 0.3467\n",
      "epoch: 2, batch_id: 1500, loss is: 0.4494\n",
      "epoch: 2, batch_id: 2000, loss is: 0.3334\n",
      "epoch: 2, batch_id: 2500, loss is: 0.2296\n",
      "epoch: 2, batch_id: 3000, loss is: 0.4526\n",
      "epoch: 2, batch_id: 3500, loss is: 0.2183\n",
      "epoch: 2, batch_id: 4000, loss is: 0.3674\n",
      "epoch: 2, batch_id: 4500, loss is: 0.1219\n",
      "epoch: 2, batch_id: 5000, loss is: 0.3918\n",
      "epoch: 2, batch_id: 5500, loss is: 0.5465\n",
      "[validation] accuracy/loss: 0.9060/0.3168\n",
      "epoch: 3, batch_id:    0, loss is: 0.8381\n",
      "epoch: 3, batch_id:  500, loss is: 0.0864\n",
      "epoch: 3, batch_id: 1000, loss is: 0.5804\n",
      "epoch: 3, batch_id: 1500, loss is: 0.3378\n",
      "epoch: 3, batch_id: 2000, loss is: 0.7960\n",
      "epoch: 3, batch_id: 2500, loss is: 0.0624\n",
      "epoch: 3, batch_id: 3000, loss is: 0.1430\n",
      "epoch: 3, batch_id: 3500, loss is: 0.3790\n",
      "epoch: 3, batch_id: 4000, loss is: 0.0921\n",
      "epoch: 3, batch_id: 4500, loss is: 0.7064\n",
      "epoch: 3, batch_id: 5000, loss is: 0.1097\n",
      "epoch: 3, batch_id: 5500, loss is: 0.2587\n",
      "[validation] accuracy/loss: 0.9272/0.2411\n",
      "epoch: 4, batch_id:    0, loss is: 1.1205\n",
      "epoch: 4, batch_id:  500, loss is: 0.2278\n",
      "epoch: 4, batch_id: 1000, loss is: 0.0500\n",
      "epoch: 4, batch_id: 1500, loss is: 0.0996\n",
      "epoch: 4, batch_id: 2000, loss is: 0.1958\n",
      "epoch: 4, batch_id: 2500, loss is: 0.1572\n",
      "epoch: 4, batch_id: 3000, loss is: 0.0712\n",
      "epoch: 4, batch_id: 3500, loss is: 0.0395\n",
      "epoch: 4, batch_id: 4000, loss is: 0.2419\n",
      "epoch: 4, batch_id: 4500, loss is: 0.0526\n",
      "epoch: 4, batch_id: 5000, loss is: 0.2715\n",
      "epoch: 4, batch_id: 5500, loss is: 0.0923\n",
      "[validation] accuracy/loss: 0.9449/0.1856\n",
      "epoch: 5, batch_id:    0, loss is: 0.1012\n",
      "epoch: 5, batch_id:  500, loss is: 0.2575\n",
      "epoch: 5, batch_id: 1000, loss is: 0.3160\n",
      "epoch: 5, batch_id: 1500, loss is: 0.1676\n",
      "epoch: 5, batch_id: 2000, loss is: 0.4440\n",
      "epoch: 5, batch_id: 2500, loss is: 0.0848\n",
      "epoch: 5, batch_id: 3000, loss is: 0.0483\n",
      "epoch: 5, batch_id: 3500, loss is: 0.1669\n",
      "epoch: 5, batch_id: 4000, loss is: 0.0158\n",
      "epoch: 5, batch_id: 4500, loss is: 0.0236\n",
      "epoch: 5, batch_id: 5000, loss is: 0.0324\n",
      "epoch: 5, batch_id: 5500, loss is: 0.3507\n",
      "[validation] accuracy/loss: 0.9528/0.1614\n",
      "epoch: 6, batch_id:    0, loss is: 0.2853\n",
      "epoch: 6, batch_id:  500, loss is: 0.0190\n",
      "epoch: 6, batch_id: 1000, loss is: 0.1386\n",
      "epoch: 6, batch_id: 1500, loss is: 0.0798\n",
      "epoch: 6, batch_id: 2000, loss is: 0.2074\n",
      "epoch: 6, batch_id: 2500, loss is: 0.0225\n",
      "epoch: 6, batch_id: 3000, loss is: 0.1426\n",
      "epoch: 6, batch_id: 3500, loss is: 0.0241\n",
      "epoch: 6, batch_id: 4000, loss is: 0.1943\n",
      "epoch: 6, batch_id: 4500, loss is: 0.1939\n",
      "epoch: 6, batch_id: 5000, loss is: 0.0270\n",
      "epoch: 6, batch_id: 5500, loss is: 0.0458\n",
      "[validation] accuracy/loss: 0.9617/0.1303\n",
      "epoch: 7, batch_id:    0, loss is: 0.0517\n",
      "epoch: 7, batch_id:  500, loss is: 0.2742\n",
      "epoch: 7, batch_id: 1000, loss is: 0.4755\n",
      "epoch: 7, batch_id: 1500, loss is: 0.1214\n",
      "epoch: 7, batch_id: 2000, loss is: 0.4382\n",
      "epoch: 7, batch_id: 2500, loss is: 0.8365\n",
      "epoch: 7, batch_id: 3000, loss is: 0.2232\n",
      "epoch: 7, batch_id: 3500, loss is: 0.5755\n",
      "epoch: 7, batch_id: 4000, loss is: 0.3362\n",
      "epoch: 7, batch_id: 4500, loss is: 0.0601\n",
      "epoch: 7, batch_id: 5000, loss is: 0.0179\n",
      "epoch: 7, batch_id: 5500, loss is: 0.0254\n",
      "[validation] accuracy/loss: 0.9642/0.1179\n",
      "epoch: 8, batch_id:    0, loss is: 0.0505\n",
      "epoch: 8, batch_id:  500, loss is: 0.0806\n",
      "epoch: 8, batch_id: 1000, loss is: 0.0153\n",
      "epoch: 8, batch_id: 1500, loss is: 0.0296\n",
      "epoch: 8, batch_id: 2000, loss is: 0.0351\n",
      "epoch: 8, batch_id: 2500, loss is: 0.0262\n",
      "epoch: 8, batch_id: 3000, loss is: 0.3028\n",
      "epoch: 8, batch_id: 3500, loss is: 0.1900\n",
      "epoch: 8, batch_id: 4000, loss is: 0.0063\n",
      "epoch: 8, batch_id: 4500, loss is: 0.0098\n",
      "epoch: 8, batch_id: 5000, loss is: 0.0287\n",
      "epoch: 8, batch_id: 5500, loss is: 0.0205\n",
      "[validation] accuracy/loss: 0.9689/0.1073\n",
      "epoch: 9, batch_id:    0, loss is: 0.0532\n",
      "epoch: 9, batch_id:  500, loss is: 0.0143\n",
      "epoch: 9, batch_id: 1000, loss is: 0.1493\n",
      "epoch: 9, batch_id: 1500, loss is: 0.0157\n",
      "epoch: 9, batch_id: 2000, loss is: 0.0561\n",
      "epoch: 9, batch_id: 2500, loss is: 0.0135\n",
      "epoch: 9, batch_id: 3000, loss is: 0.3744\n",
      "epoch: 9, batch_id: 3500, loss is: 0.3100\n",
      "epoch: 9, batch_id: 4000, loss is: 0.0199\n",
      "epoch: 9, batch_id: 4500, loss is: 0.0255\n",
      "epoch: 9, batch_id: 5000, loss is: 0.1406\n",
      "epoch: 9, batch_id: 5500, loss is: 0.0874\n",
      "[validation] accuracy/loss: 0.9706/0.0950\n",
      "epoch: 10, batch_id:    0, loss is: 0.0619\n",
      "epoch: 10, batch_id:  500, loss is: 0.0307\n",
      "epoch: 10, batch_id: 1000, loss is: 0.1407\n",
      "epoch: 10, batch_id: 1500, loss is: 0.0099\n",
      "epoch: 10, batch_id: 2000, loss is: 0.0805\n",
      "epoch: 10, batch_id: 2500, loss is: 0.0110\n",
      "epoch: 10, batch_id: 3000, loss is: 0.0924\n",
      "epoch: 10, batch_id: 3500, loss is: 0.0169\n",
      "epoch: 10, batch_id: 4000, loss is: 0.0138\n",
      "epoch: 10, batch_id: 4500, loss is: 0.2079\n",
      "epoch: 10, batch_id: 5000, loss is: 0.0469\n",
      "epoch: 10, batch_id: 5500, loss is: 0.2229\n",
      "[validation] accuracy/loss: 0.9734/0.0860\n",
      "epoch: 11, batch_id:    0, loss is: 0.0193\n",
      "epoch: 11, batch_id:  500, loss is: 0.0193\n",
      "epoch: 11, batch_id: 1000, loss is: 0.0064\n",
      "epoch: 11, batch_id: 1500, loss is: 0.0106\n",
      "epoch: 11, batch_id: 2000, loss is: 0.1817\n",
      "epoch: 11, batch_id: 2500, loss is: 0.0931\n",
      "epoch: 11, batch_id: 3000, loss is: 0.0104\n",
      "epoch: 11, batch_id: 3500, loss is: 0.0403\n",
      "epoch: 11, batch_id: 4000, loss is: 0.1751\n",
      "epoch: 11, batch_id: 4500, loss is: 0.0087\n",
      "epoch: 11, batch_id: 5000, loss is: 0.0143\n",
      "epoch: 11, batch_id: 5500, loss is: 0.0067\n",
      "[validation] accuracy/loss: 0.9751/0.0827\n",
      "epoch: 12, batch_id:    0, loss is: 0.0086\n",
      "epoch: 12, batch_id:  500, loss is: 0.1522\n",
      "epoch: 12, batch_id: 1000, loss is: 0.0224\n",
      "epoch: 12, batch_id: 1500, loss is: 0.0697\n",
      "epoch: 12, batch_id: 2000, loss is: 0.5661\n",
      "epoch: 12, batch_id: 2500, loss is: 0.0480\n",
      "epoch: 12, batch_id: 3000, loss is: 0.0996\n",
      "epoch: 12, batch_id: 3500, loss is: 0.0112\n",
      "epoch: 12, batch_id: 4000, loss is: 0.0860\n",
      "epoch: 12, batch_id: 4500, loss is: 0.0175\n",
      "epoch: 12, batch_id: 5000, loss is: 0.0101\n",
      "epoch: 12, batch_id: 5500, loss is: 0.0088\n",
      "[validation] accuracy/loss: 0.9743/0.0811\n",
      "epoch: 13, batch_id:    0, loss is: 0.0062\n",
      "epoch: 13, batch_id:  500, loss is: 0.0049\n",
      "epoch: 13, batch_id: 1000, loss is: 0.1596\n",
      "epoch: 13, batch_id: 1500, loss is: 0.0066\n",
      "epoch: 13, batch_id: 2000, loss is: 0.0897\n",
      "epoch: 13, batch_id: 2500, loss is: 0.1235\n",
      "epoch: 13, batch_id: 3000, loss is: 0.0115\n",
      "epoch: 13, batch_id: 3500, loss is: 0.0083\n",
      "epoch: 13, batch_id: 4000, loss is: 0.0626\n",
      "epoch: 13, batch_id: 4500, loss is: 0.0844\n",
      "epoch: 13, batch_id: 5000, loss is: 0.0827\n",
      "epoch: 13, batch_id: 5500, loss is: 0.1286\n",
      "[validation] accuracy/loss: 0.9758/0.0790\n",
      "epoch: 14, batch_id:    0, loss is: 0.0193\n",
      "epoch: 14, batch_id:  500, loss is: 0.0157\n",
      "epoch: 14, batch_id: 1000, loss is: 0.0176\n",
      "epoch: 14, batch_id: 1500, loss is: 0.0066\n",
      "epoch: 14, batch_id: 2000, loss is: 0.0292\n",
      "epoch: 14, batch_id: 2500, loss is: 0.0230\n",
      "epoch: 14, batch_id: 3000, loss is: 0.0094\n",
      "epoch: 14, batch_id: 3500, loss is: 0.0502\n",
      "epoch: 14, batch_id: 4000, loss is: 0.0242\n",
      "epoch: 14, batch_id: 4500, loss is: 0.1410\n",
      "epoch: 14, batch_id: 5000, loss is: 0.0066\n",
      "epoch: 14, batch_id: 5500, loss is: 0.0281\n",
      "[validation] accuracy/loss: 0.9772/0.0707\n",
      "epoch: 15, batch_id:    0, loss is: 0.0079\n",
      "epoch: 15, batch_id:  500, loss is: 0.0045\n",
      "epoch: 15, batch_id: 1000, loss is: 0.0274\n",
      "epoch: 15, batch_id: 1500, loss is: 0.0388\n",
      "epoch: 15, batch_id: 2000, loss is: 0.1127\n",
      "epoch: 15, batch_id: 2500, loss is: 0.0021\n",
      "epoch: 15, batch_id: 3000, loss is: 0.0240\n",
      "epoch: 15, batch_id: 3500, loss is: 0.0060\n",
      "epoch: 15, batch_id: 4000, loss is: 0.2242\n",
      "epoch: 15, batch_id: 4500, loss is: 0.0118\n",
      "epoch: 15, batch_id: 5000, loss is: 0.0048\n",
      "epoch: 15, batch_id: 5500, loss is: 0.0966\n",
      "[validation] accuracy/loss: 0.9790/0.0691\n",
      "epoch: 16, batch_id:    0, loss is: 0.4990\n",
      "epoch: 16, batch_id:  500, loss is: 0.0261\n",
      "epoch: 16, batch_id: 1000, loss is: 0.0701\n",
      "epoch: 16, batch_id: 1500, loss is: 0.0768\n",
      "epoch: 16, batch_id: 2000, loss is: 0.0025\n",
      "epoch: 16, batch_id: 2500, loss is: 0.1897\n",
      "epoch: 16, batch_id: 3000, loss is: 0.0247\n",
      "epoch: 16, batch_id: 3500, loss is: 0.0337\n",
      "epoch: 16, batch_id: 4000, loss is: 0.0167\n",
      "epoch: 16, batch_id: 4500, loss is: 0.0071\n",
      "epoch: 16, batch_id: 5000, loss is: 0.0168\n",
      "epoch: 16, batch_id: 5500, loss is: 0.0052\n",
      "[validation] accuracy/loss: 0.9791/0.0653\n",
      "epoch: 17, batch_id:    0, loss is: 0.0074\n",
      "epoch: 17, batch_id:  500, loss is: 0.0025\n",
      "epoch: 17, batch_id: 1000, loss is: 0.0187\n",
      "epoch: 17, batch_id: 1500, loss is: 0.0664\n",
      "epoch: 17, batch_id: 2000, loss is: 0.0399\n",
      "epoch: 17, batch_id: 2500, loss is: 0.0526\n",
      "epoch: 17, batch_id: 3000, loss is: 0.0867\n",
      "epoch: 17, batch_id: 3500, loss is: 0.2208\n",
      "epoch: 17, batch_id: 4000, loss is: 0.0483\n",
      "epoch: 17, batch_id: 4500, loss is: 0.0042\n",
      "epoch: 17, batch_id: 5000, loss is: 0.0039\n",
      "epoch: 17, batch_id: 5500, loss is: 0.0072\n",
      "[validation] accuracy/loss: 0.9767/0.0711\n",
      "epoch: 18, batch_id:    0, loss is: 0.0394\n",
      "epoch: 18, batch_id:  500, loss is: 0.0159\n",
      "epoch: 18, batch_id: 1000, loss is: 0.0207\n",
      "epoch: 18, batch_id: 1500, loss is: 0.0144\n",
      "epoch: 18, batch_id: 2000, loss is: 0.0356\n",
      "epoch: 18, batch_id: 2500, loss is: 0.2769\n",
      "epoch: 18, batch_id: 3000, loss is: 0.0162\n",
      "epoch: 18, batch_id: 3500, loss is: 0.0941\n",
      "epoch: 18, batch_id: 4000, loss is: 0.0029\n",
      "epoch: 18, batch_id: 4500, loss is: 0.0264\n",
      "epoch: 18, batch_id: 5000, loss is: 0.1510\n",
      "epoch: 18, batch_id: 5500, loss is: 0.0058\n",
      "[validation] accuracy/loss: 0.9812/0.0612\n",
      "epoch: 19, batch_id:    0, loss is: 0.0039\n",
      "epoch: 19, batch_id:  500, loss is: 0.0159\n",
      "epoch: 19, batch_id: 1000, loss is: 0.0068\n",
      "epoch: 19, batch_id: 1500, loss is: 0.0208\n",
      "epoch: 19, batch_id: 2000, loss is: 0.0021\n",
      "epoch: 19, batch_id: 2500, loss is: 0.0217\n",
      "epoch: 19, batch_id: 3000, loss is: 0.0027\n",
      "epoch: 19, batch_id: 3500, loss is: 0.0424\n",
      "epoch: 19, batch_id: 4000, loss is: 0.1620\n",
      "epoch: 19, batch_id: 4500, loss is: 0.0085\n",
      "epoch: 19, batch_id: 5000, loss is: 0.0109\n",
      "epoch: 19, batch_id: 5500, loss is: 0.2017\n",
      "[validation] accuracy/loss: 0.9808/0.0589\n",
      "epoch: 20, batch_id:    0, loss is: 0.1677\n",
      "epoch: 20, batch_id:  500, loss is: 0.0031\n",
      "epoch: 20, batch_id: 1000, loss is: 0.0147\n",
      "epoch: 20, batch_id: 1500, loss is: 0.1480\n",
      "epoch: 20, batch_id: 2000, loss is: 0.1033\n",
      "epoch: 20, batch_id: 2500, loss is: 0.0012\n",
      "epoch: 20, batch_id: 3000, loss is: 0.0040\n",
      "epoch: 20, batch_id: 3500, loss is: 0.0243\n",
      "epoch: 20, batch_id: 4000, loss is: 0.0164\n",
      "epoch: 20, batch_id: 4500, loss is: 0.0053\n",
      "epoch: 20, batch_id: 5000, loss is: 0.1937\n",
      "epoch: 20, batch_id: 5500, loss is: 0.0345\n",
      "[validation] accuracy/loss: 0.9810/0.0604\n",
      "epoch: 21, batch_id:    0, loss is: 0.0159\n",
      "epoch: 21, batch_id:  500, loss is: 0.0094\n",
      "epoch: 21, batch_id: 1000, loss is: 0.0026\n",
      "epoch: 21, batch_id: 1500, loss is: 0.0008\n",
      "epoch: 21, batch_id: 2000, loss is: 0.0360\n",
      "epoch: 21, batch_id: 2500, loss is: 0.0404\n",
      "epoch: 21, batch_id: 3000, loss is: 0.0054\n",
      "epoch: 21, batch_id: 3500, loss is: 0.0732\n",
      "epoch: 21, batch_id: 4000, loss is: 0.1821\n",
      "epoch: 21, batch_id: 4500, loss is: 0.0812\n",
      "epoch: 21, batch_id: 5000, loss is: 0.0976\n",
      "epoch: 21, batch_id: 5500, loss is: 0.2433\n",
      "[validation] accuracy/loss: 0.9812/0.0570\n",
      "epoch: 22, batch_id:    0, loss is: 0.0076\n",
      "epoch: 22, batch_id:  500, loss is: 0.0553\n",
      "epoch: 22, batch_id: 1000, loss is: 0.0191\n",
      "epoch: 22, batch_id: 1500, loss is: 0.0165\n",
      "epoch: 22, batch_id: 2000, loss is: 0.0715\n",
      "epoch: 22, batch_id: 2500, loss is: 0.0739\n",
      "epoch: 22, batch_id: 3000, loss is: 0.0059\n",
      "epoch: 22, batch_id: 3500, loss is: 0.0169\n",
      "epoch: 22, batch_id: 4000, loss is: 0.0256\n",
      "epoch: 22, batch_id: 4500, loss is: 0.0100\n",
      "epoch: 22, batch_id: 5000, loss is: 0.0119\n",
      "epoch: 22, batch_id: 5500, loss is: 0.1413\n",
      "[validation] accuracy/loss: 0.9808/0.0586\n",
      "epoch: 23, batch_id:    0, loss is: 0.0030\n",
      "epoch: 23, batch_id:  500, loss is: 0.0187\n",
      "epoch: 23, batch_id: 1000, loss is: 0.0066\n",
      "epoch: 23, batch_id: 1500, loss is: 0.0263\n",
      "epoch: 23, batch_id: 2000, loss is: 0.0148\n",
      "epoch: 23, batch_id: 2500, loss is: 0.0017\n",
      "epoch: 23, batch_id: 3000, loss is: 0.0042\n",
      "epoch: 23, batch_id: 3500, loss is: 0.0125\n",
      "epoch: 23, batch_id: 4000, loss is: 0.0009\n",
      "epoch: 23, batch_id: 4500, loss is: 0.0540\n",
      "epoch: 23, batch_id: 5000, loss is: 0.1820\n",
      "epoch: 23, batch_id: 5500, loss is: 0.0035\n",
      "[validation] accuracy/loss: 0.9837/0.0516\n",
      "epoch: 24, batch_id:    0, loss is: 0.0028\n",
      "epoch: 24, batch_id:  500, loss is: 0.0065\n",
      "epoch: 24, batch_id: 1000, loss is: 0.5867\n",
      "epoch: 24, batch_id: 1500, loss is: 0.0127\n",
      "epoch: 24, batch_id: 2000, loss is: 0.0102\n",
      "epoch: 24, batch_id: 2500, loss is: 0.0050\n",
      "epoch: 24, batch_id: 3000, loss is: 0.1698\n",
      "epoch: 24, batch_id: 3500, loss is: 0.0284\n",
      "epoch: 24, batch_id: 4000, loss is: 0.0147\n",
      "epoch: 24, batch_id: 4500, loss is: 0.0325\n",
      "epoch: 24, batch_id: 5000, loss is: 0.0927\n",
      "epoch: 24, batch_id: 5500, loss is: 0.0092\n",
      "[validation] accuracy/loss: 0.9832/0.0533\n",
      "epoch: 25, batch_id:    0, loss is: 0.0617\n",
      "epoch: 25, batch_id:  500, loss is: 0.1447\n",
      "epoch: 25, batch_id: 1000, loss is: 0.0164\n",
      "epoch: 25, batch_id: 1500, loss is: 0.0066\n",
      "epoch: 25, batch_id: 2000, loss is: 0.0026\n",
      "epoch: 25, batch_id: 2500, loss is: 0.2302\n",
      "epoch: 25, batch_id: 3000, loss is: 0.0022\n",
      "epoch: 25, batch_id: 3500, loss is: 0.2112\n",
      "epoch: 25, batch_id: 4000, loss is: 0.0046\n",
      "epoch: 25, batch_id: 4500, loss is: 0.0072\n",
      "epoch: 25, batch_id: 5000, loss is: 0.1418\n",
      "epoch: 25, batch_id: 5500, loss is: 0.0156\n",
      "[validation] accuracy/loss: 0.9839/0.0506\n",
      "epoch: 26, batch_id:    0, loss is: 0.0137\n",
      "epoch: 26, batch_id:  500, loss is: 0.0903\n",
      "epoch: 26, batch_id: 1000, loss is: 0.0429\n",
      "epoch: 26, batch_id: 1500, loss is: 0.0645\n",
      "epoch: 26, batch_id: 2000, loss is: 0.0111\n",
      "epoch: 26, batch_id: 2500, loss is: 0.0194\n",
      "epoch: 26, batch_id: 3000, loss is: 0.0816\n",
      "epoch: 26, batch_id: 3500, loss is: 0.1880\n",
      "epoch: 26, batch_id: 4000, loss is: 0.0020\n",
      "epoch: 26, batch_id: 4500, loss is: 0.0054\n",
      "epoch: 26, batch_id: 5000, loss is: 0.1273\n",
      "epoch: 26, batch_id: 5500, loss is: 0.0432\n",
      "[validation] accuracy/loss: 0.9833/0.0505\n",
      "epoch: 27, batch_id:    0, loss is: 0.0142\n",
      "epoch: 27, batch_id:  500, loss is: 0.0038\n",
      "epoch: 27, batch_id: 1000, loss is: 0.0014\n",
      "epoch: 27, batch_id: 1500, loss is: 0.0086\n",
      "epoch: 27, batch_id: 2000, loss is: 0.0162\n",
      "epoch: 27, batch_id: 2500, loss is: 0.0029\n",
      "epoch: 27, batch_id: 3000, loss is: 0.3508\n",
      "epoch: 27, batch_id: 3500, loss is: 0.0017\n",
      "epoch: 27, batch_id: 4000, loss is: 0.0204\n",
      "epoch: 27, batch_id: 4500, loss is: 0.0039\n",
      "epoch: 27, batch_id: 5000, loss is: 0.0110\n",
      "epoch: 27, batch_id: 5500, loss is: 0.0034\n",
      "[validation] accuracy/loss: 0.9840/0.0507\n",
      "epoch: 28, batch_id:    0, loss is: 0.0496\n",
      "epoch: 28, batch_id:  500, loss is: 0.1171\n",
      "epoch: 28, batch_id: 1000, loss is: 0.0741\n",
      "epoch: 28, batch_id: 1500, loss is: 0.0220\n",
      "epoch: 28, batch_id: 2000, loss is: 0.1414\n",
      "epoch: 28, batch_id: 2500, loss is: 0.0067\n",
      "epoch: 28, batch_id: 3000, loss is: 0.0101\n",
      "epoch: 28, batch_id: 3500, loss is: 0.0431\n",
      "epoch: 28, batch_id: 4000, loss is: 0.0512\n",
      "epoch: 28, batch_id: 4500, loss is: 0.0623\n",
      "epoch: 28, batch_id: 5000, loss is: 0.0108\n",
      "epoch: 28, batch_id: 5500, loss is: 0.0154\n",
      "[validation] accuracy/loss: 0.9840/0.0488\n",
      "epoch: 29, batch_id:    0, loss is: 0.0034\n",
      "epoch: 29, batch_id:  500, loss is: 0.0538\n",
      "epoch: 29, batch_id: 1000, loss is: 0.0014\n",
      "epoch: 29, batch_id: 1500, loss is: 0.0296\n",
      "epoch: 29, batch_id: 2000, loss is: 0.0217\n",
      "epoch: 29, batch_id: 2500, loss is: 0.0921\n",
      "epoch: 29, batch_id: 3000, loss is: 0.0585\n",
      "epoch: 29, batch_id: 3500, loss is: 0.0013\n",
      "epoch: 29, batch_id: 4000, loss is: 0.0091\n",
      "epoch: 29, batch_id: 4500, loss is: 0.0035\n",
      "epoch: 29, batch_id: 5000, loss is: 0.0178\n",
      "epoch: 29, batch_id: 5500, loss is: 0.0023\n",
      "[validation] accuracy/loss: 0.9838/0.0501\n",
      "epoch: 30, batch_id:    0, loss is: 0.0092\n",
      "epoch: 30, batch_id:  500, loss is: 0.0256\n",
      "epoch: 30, batch_id: 1000, loss is: 0.0713\n",
      "epoch: 30, batch_id: 1500, loss is: 0.0062\n",
      "epoch: 30, batch_id: 2000, loss is: 0.0039\n",
      "epoch: 30, batch_id: 2500, loss is: 0.0095\n",
      "epoch: 30, batch_id: 3000, loss is: 0.0084\n",
      "epoch: 30, batch_id: 3500, loss is: 0.0190\n",
      "epoch: 30, batch_id: 4000, loss is: 0.0028\n",
      "epoch: 30, batch_id: 4500, loss is: 0.0248\n",
      "epoch: 30, batch_id: 5000, loss is: 0.0354\n",
      "epoch: 30, batch_id: 5500, loss is: 0.0074\n",
      "[validation] accuracy/loss: 0.9856/0.0470\n",
      "epoch: 31, batch_id:    0, loss is: 0.0064\n",
      "epoch: 31, batch_id:  500, loss is: 0.0051\n",
      "epoch: 31, batch_id: 1000, loss is: 0.0049\n",
      "epoch: 31, batch_id: 1500, loss is: 0.0055\n",
      "epoch: 31, batch_id: 2000, loss is: 0.0045\n",
      "epoch: 31, batch_id: 2500, loss is: 0.4360\n",
      "epoch: 31, batch_id: 3000, loss is: 0.0013\n",
      "epoch: 31, batch_id: 3500, loss is: 0.0043\n",
      "epoch: 31, batch_id: 4000, loss is: 0.0767\n",
      "epoch: 31, batch_id: 4500, loss is: 0.0343\n",
      "epoch: 31, batch_id: 5000, loss is: 0.0019\n",
      "epoch: 31, batch_id: 5500, loss is: 0.0056\n",
      "[validation] accuracy/loss: 0.9854/0.0473\n",
      "epoch: 32, batch_id:    0, loss is: 0.0024\n",
      "epoch: 32, batch_id:  500, loss is: 0.0006\n",
      "epoch: 32, batch_id: 1000, loss is: 0.0039\n",
      "epoch: 32, batch_id: 1500, loss is: 0.0046\n",
      "epoch: 32, batch_id: 2000, loss is: 0.0121\n",
      "epoch: 32, batch_id: 2500, loss is: 0.0052\n",
      "epoch: 32, batch_id: 3000, loss is: 0.0172\n",
      "epoch: 32, batch_id: 3500, loss is: 0.0074\n",
      "epoch: 32, batch_id: 4000, loss is: 0.1321\n",
      "epoch: 32, batch_id: 4500, loss is: 0.0267\n",
      "epoch: 32, batch_id: 5000, loss is: 0.0149\n",
      "epoch: 32, batch_id: 5500, loss is: 0.0006\n",
      "[validation] accuracy/loss: 0.9855/0.0457\n",
      "epoch: 33, batch_id:    0, loss is: 0.2226\n",
      "epoch: 33, batch_id:  500, loss is: 0.0180\n",
      "epoch: 33, batch_id: 1000, loss is: 0.0645\n",
      "epoch: 33, batch_id: 1500, loss is: 0.0150\n",
      "epoch: 33, batch_id: 2000, loss is: 0.0182\n",
      "epoch: 33, batch_id: 2500, loss is: 0.0234\n",
      "epoch: 33, batch_id: 3000, loss is: 0.0009\n",
      "epoch: 33, batch_id: 3500, loss is: 0.3951\n",
      "epoch: 33, batch_id: 4000, loss is: 0.0026\n",
      "epoch: 33, batch_id: 4500, loss is: 0.1030\n",
      "epoch: 33, batch_id: 5000, loss is: 0.0050\n",
      "epoch: 33, batch_id: 5500, loss is: 0.0011\n",
      "[validation] accuracy/loss: 0.9851/0.0471\n",
      "epoch: 34, batch_id:    0, loss is: 0.0031\n",
      "epoch: 34, batch_id:  500, loss is: 0.0027\n",
      "epoch: 34, batch_id: 1000, loss is: 0.0200\n",
      "epoch: 34, batch_id: 1500, loss is: 0.9991\n",
      "epoch: 34, batch_id: 2000, loss is: 0.0667\n",
      "epoch: 34, batch_id: 2500, loss is: 0.0109\n",
      "epoch: 34, batch_id: 3000, loss is: 0.0005\n",
      "epoch: 34, batch_id: 3500, loss is: 0.0749\n",
      "epoch: 34, batch_id: 4000, loss is: 0.0003\n",
      "epoch: 34, batch_id: 4500, loss is: 0.0033\n",
      "epoch: 34, batch_id: 5000, loss is: 0.0079\n",
      "epoch: 34, batch_id: 5500, loss is: 0.0291\n",
      "[validation] accuracy/loss: 0.9848/0.0457\n",
      "epoch: 35, batch_id:    0, loss is: 0.0251\n",
      "epoch: 35, batch_id:  500, loss is: 0.0026\n",
      "epoch: 35, batch_id: 1000, loss is: 0.1550\n",
      "epoch: 35, batch_id: 1500, loss is: 0.0125\n",
      "epoch: 35, batch_id: 2000, loss is: 0.0469\n",
      "epoch: 35, batch_id: 2500, loss is: 0.0012\n",
      "epoch: 35, batch_id: 3000, loss is: 0.1920\n",
      "epoch: 35, batch_id: 3500, loss is: 0.1421\n",
      "epoch: 35, batch_id: 4000, loss is: 0.0044\n",
      "epoch: 35, batch_id: 4500, loss is: 0.0012\n",
      "epoch: 35, batch_id: 5000, loss is: 0.0024\n",
      "epoch: 35, batch_id: 5500, loss is: 0.0007\n",
      "[validation] accuracy/loss: 0.9851/0.0469\n",
      "epoch: 36, batch_id:    0, loss is: 0.0033\n",
      "epoch: 36, batch_id:  500, loss is: 0.0143\n",
      "epoch: 36, batch_id: 1000, loss is: 0.0031\n",
      "epoch: 36, batch_id: 1500, loss is: 0.0048\n",
      "epoch: 36, batch_id: 2000, loss is: 0.0009\n",
      "epoch: 36, batch_id: 2500, loss is: 0.0087\n",
      "epoch: 36, batch_id: 3000, loss is: 0.0274\n",
      "epoch: 36, batch_id: 3500, loss is: 0.0010\n",
      "epoch: 36, batch_id: 4000, loss is: 0.0133\n",
      "epoch: 36, batch_id: 4500, loss is: 0.0057\n",
      "epoch: 36, batch_id: 5000, loss is: 0.0240\n",
      "epoch: 36, batch_id: 5500, loss is: 0.0016\n",
      "[validation] accuracy/loss: 0.9843/0.0467\n",
      "epoch: 37, batch_id:    0, loss is: 0.0719\n",
      "epoch: 37, batch_id:  500, loss is: 0.0016\n",
      "epoch: 37, batch_id: 1000, loss is: 0.0243\n",
      "epoch: 37, batch_id: 1500, loss is: 0.0131\n",
      "epoch: 37, batch_id: 2000, loss is: 0.0109\n",
      "epoch: 37, batch_id: 2500, loss is: 0.0023\n",
      "epoch: 37, batch_id: 3000, loss is: 0.0473\n",
      "epoch: 37, batch_id: 3500, loss is: 0.0032\n",
      "epoch: 37, batch_id: 4000, loss is: 0.0026\n",
      "epoch: 37, batch_id: 4500, loss is: 0.0015\n",
      "epoch: 37, batch_id: 5000, loss is: 0.0010\n",
      "epoch: 37, batch_id: 5500, loss is: 0.0626\n",
      "[validation] accuracy/loss: 0.9844/0.0462\n",
      "epoch: 38, batch_id:    0, loss is: 0.0254\n",
      "epoch: 38, batch_id:  500, loss is: 0.0020\n",
      "epoch: 38, batch_id: 1000, loss is: 0.0012\n",
      "epoch: 38, batch_id: 1500, loss is: 0.0068\n",
      "epoch: 38, batch_id: 2000, loss is: 0.0003\n",
      "epoch: 38, batch_id: 2500, loss is: 0.0289\n",
      "epoch: 38, batch_id: 3000, loss is: 0.0012\n",
      "epoch: 38, batch_id: 3500, loss is: 0.0037\n",
      "epoch: 38, batch_id: 4000, loss is: 0.0079\n",
      "epoch: 38, batch_id: 4500, loss is: 0.0056\n",
      "epoch: 38, batch_id: 5000, loss is: 0.0002\n",
      "epoch: 38, batch_id: 5500, loss is: 0.0025\n",
      "[validation] accuracy/loss: 0.9856/0.0452\n",
      "epoch: 39, batch_id:    0, loss is: 0.1432\n",
      "epoch: 39, batch_id:  500, loss is: 0.0076\n",
      "epoch: 39, batch_id: 1000, loss is: 0.1186\n",
      "epoch: 39, batch_id: 1500, loss is: 0.0023\n",
      "epoch: 39, batch_id: 2000, loss is: 0.0017\n",
      "epoch: 39, batch_id: 2500, loss is: 0.0007\n",
      "epoch: 39, batch_id: 3000, loss is: 0.1109\n",
      "epoch: 39, batch_id: 3500, loss is: 0.0351\n",
      "epoch: 39, batch_id: 4000, loss is: 0.0006\n",
      "epoch: 39, batch_id: 4500, loss is: 0.0120\n",
      "epoch: 39, batch_id: 5000, loss is: 0.0219\n",
      "epoch: 39, batch_id: 5500, loss is: 0.1681\n",
      "[validation] accuracy/loss: 0.9857/0.0438\n",
      "epoch: 40, batch_id:    0, loss is: 0.0149\n",
      "epoch: 40, batch_id:  500, loss is: 0.0131\n",
      "epoch: 40, batch_id: 1000, loss is: 0.0017\n",
      "epoch: 40, batch_id: 1500, loss is: 0.0011\n",
      "epoch: 40, batch_id: 2000, loss is: 0.1796\n",
      "epoch: 40, batch_id: 2500, loss is: 0.0041\n",
      "epoch: 40, batch_id: 3000, loss is: 0.0009\n",
      "epoch: 40, batch_id: 3500, loss is: 0.0108\n",
      "epoch: 40, batch_id: 4000, loss is: 0.0159\n",
      "epoch: 40, batch_id: 4500, loss is: 0.0752\n",
      "epoch: 40, batch_id: 5000, loss is: 0.0409\n",
      "epoch: 40, batch_id: 5500, loss is: 0.0006\n",
      "[validation] accuracy/loss: 0.9857/0.0446\n",
      "epoch: 41, batch_id:    0, loss is: 0.1895\n",
      "epoch: 41, batch_id:  500, loss is: 0.0002\n",
      "epoch: 41, batch_id: 1000, loss is: 0.0054\n",
      "epoch: 41, batch_id: 1500, loss is: 0.0044\n",
      "epoch: 41, batch_id: 2000, loss is: 0.1391\n",
      "epoch: 41, batch_id: 2500, loss is: 0.0119\n",
      "epoch: 41, batch_id: 3000, loss is: 0.0118\n",
      "epoch: 41, batch_id: 3500, loss is: 0.0003\n",
      "epoch: 41, batch_id: 4000, loss is: 0.0054\n",
      "epoch: 41, batch_id: 4500, loss is: 0.0029\n",
      "epoch: 41, batch_id: 5000, loss is: 0.0303\n",
      "epoch: 41, batch_id: 5500, loss is: 0.0116\n",
      "[validation] accuracy/loss: 0.9864/0.0421\n",
      "epoch: 42, batch_id:    0, loss is: 0.0435\n",
      "epoch: 42, batch_id:  500, loss is: 0.0344\n",
      "epoch: 42, batch_id: 1000, loss is: 0.0147\n",
      "epoch: 42, batch_id: 1500, loss is: 0.0128\n",
      "epoch: 42, batch_id: 2000, loss is: 0.0702\n",
      "epoch: 42, batch_id: 2500, loss is: 0.0083\n",
      "epoch: 42, batch_id: 3000, loss is: 0.0017\n",
      "epoch: 42, batch_id: 3500, loss is: 0.0469\n",
      "epoch: 42, batch_id: 4000, loss is: 0.0146\n",
      "epoch: 42, batch_id: 4500, loss is: 0.0062\n",
      "epoch: 42, batch_id: 5000, loss is: 0.0046\n",
      "epoch: 42, batch_id: 5500, loss is: 0.1134\n",
      "[validation] accuracy/loss: 0.9858/0.0433\n",
      "epoch: 43, batch_id:    0, loss is: 0.0223\n",
      "epoch: 43, batch_id:  500, loss is: 0.0984\n",
      "epoch: 43, batch_id: 1000, loss is: 0.0089\n",
      "epoch: 43, batch_id: 1500, loss is: 0.0008\n",
      "epoch: 43, batch_id: 2000, loss is: 0.0002\n",
      "epoch: 43, batch_id: 2500, loss is: 0.0196\n",
      "epoch: 43, batch_id: 3000, loss is: 0.0028\n",
      "epoch: 43, batch_id: 3500, loss is: 0.0253\n",
      "epoch: 43, batch_id: 4000, loss is: 0.0875\n",
      "epoch: 43, batch_id: 4500, loss is: 0.0008\n",
      "epoch: 43, batch_id: 5000, loss is: 0.0015\n",
      "epoch: 43, batch_id: 5500, loss is: 0.0003\n",
      "[validation] accuracy/loss: 0.9860/0.0419\n",
      "epoch: 44, batch_id:    0, loss is: 0.0396\n",
      "epoch: 44, batch_id:  500, loss is: 0.0428\n",
      "epoch: 44, batch_id: 1000, loss is: 0.0992\n",
      "epoch: 44, batch_id: 1500, loss is: 0.0009\n",
      "epoch: 44, batch_id: 2000, loss is: 0.0136\n",
      "epoch: 44, batch_id: 2500, loss is: 0.0045\n",
      "epoch: 44, batch_id: 3000, loss is: 0.1954\n",
      "epoch: 44, batch_id: 3500, loss is: 0.0557\n",
      "epoch: 44, batch_id: 4000, loss is: 0.0032\n",
      "epoch: 44, batch_id: 4500, loss is: 0.0074\n",
      "epoch: 44, batch_id: 5000, loss is: 0.0522\n",
      "epoch: 44, batch_id: 5500, loss is: 0.0019\n",
      "[validation] accuracy/loss: 0.9856/0.0436\n",
      "epoch: 45, batch_id:    0, loss is: 0.0151\n",
      "epoch: 45, batch_id:  500, loss is: 0.0091\n",
      "epoch: 45, batch_id: 1000, loss is: 0.0032\n",
      "epoch: 45, batch_id: 1500, loss is: 0.1428\n",
      "epoch: 45, batch_id: 2000, loss is: 0.0290\n",
      "epoch: 45, batch_id: 2500, loss is: 0.0016\n",
      "epoch: 45, batch_id: 3000, loss is: 0.0005\n",
      "epoch: 45, batch_id: 3500, loss is: 0.0024\n",
      "epoch: 45, batch_id: 4000, loss is: 0.0124\n",
      "epoch: 45, batch_id: 4500, loss is: 0.0881\n",
      "epoch: 45, batch_id: 5000, loss is: 0.0885\n",
      "epoch: 45, batch_id: 5500, loss is: 0.0024\n",
      "[validation] accuracy/loss: 0.9847/0.0466\n",
      "epoch: 46, batch_id:    0, loss is: 0.1316\n",
      "epoch: 46, batch_id:  500, loss is: 0.0040\n",
      "epoch: 46, batch_id: 1000, loss is: 0.0311\n",
      "epoch: 46, batch_id: 1500, loss is: 0.0477\n",
      "epoch: 46, batch_id: 2000, loss is: 0.0023\n",
      "epoch: 46, batch_id: 2500, loss is: 0.0034\n",
      "epoch: 46, batch_id: 3000, loss is: 0.0017\n",
      "epoch: 46, batch_id: 3500, loss is: 0.0028\n",
      "epoch: 46, batch_id: 4000, loss is: 0.0004\n",
      "epoch: 46, batch_id: 4500, loss is: 0.0888\n",
      "epoch: 46, batch_id: 5000, loss is: 0.0097\n",
      "epoch: 46, batch_id: 5500, loss is: 0.0024\n",
      "[validation] accuracy/loss: 0.9858/0.0421\n",
      "epoch: 47, batch_id:    0, loss is: 0.0210\n",
      "epoch: 47, batch_id:  500, loss is: 0.0412\n",
      "epoch: 47, batch_id: 1000, loss is: 0.0402\n",
      "epoch: 47, batch_id: 1500, loss is: 0.0007\n",
      "epoch: 47, batch_id: 2000, loss is: 0.0020\n",
      "epoch: 47, batch_id: 2500, loss is: 0.0034\n",
      "epoch: 47, batch_id: 3000, loss is: 0.0021\n",
      "epoch: 47, batch_id: 3500, loss is: 0.0032\n",
      "epoch: 47, batch_id: 4000, loss is: 0.0021\n",
      "epoch: 47, batch_id: 4500, loss is: 0.0150\n",
      "epoch: 47, batch_id: 5000, loss is: 0.0013\n",
      "epoch: 47, batch_id: 5500, loss is: 0.0012\n",
      "[validation] accuracy/loss: 0.9858/0.0430\n",
      "epoch: 48, batch_id:    0, loss is: 0.0065\n",
      "epoch: 48, batch_id:  500, loss is: 0.0097\n",
      "epoch: 48, batch_id: 1000, loss is: 0.0137\n",
      "epoch: 48, batch_id: 1500, loss is: 0.1889\n",
      "epoch: 48, batch_id: 2000, loss is: 0.0040\n",
      "epoch: 48, batch_id: 2500, loss is: 0.0203\n",
      "epoch: 48, batch_id: 3000, loss is: 0.0302\n",
      "epoch: 48, batch_id: 3500, loss is: 0.0099\n",
      "epoch: 48, batch_id: 4000, loss is: 0.1089\n",
      "epoch: 48, batch_id: 4500, loss is: 0.0036\n",
      "epoch: 48, batch_id: 5000, loss is: 0.0065\n",
      "epoch: 48, batch_id: 5500, loss is: 0.0092\n",
      "[validation] accuracy/loss: 0.9867/0.0414\n",
      "epoch: 49, batch_id:    0, loss is: 0.0017\n",
      "epoch: 49, batch_id:  500, loss is: 0.0040\n",
      "epoch: 49, batch_id: 1000, loss is: 0.0030\n",
      "epoch: 49, batch_id: 1500, loss is: 0.0079\n",
      "epoch: 49, batch_id: 2000, loss is: 0.0018\n",
      "epoch: 49, batch_id: 2500, loss is: 0.0097\n",
      "epoch: 49, batch_id: 3000, loss is: 0.0011\n",
      "epoch: 49, batch_id: 3500, loss is: 0.0365\n",
      "epoch: 49, batch_id: 4000, loss is: 0.0002\n",
      "epoch: 49, batch_id: 4500, loss is: 0.0006\n",
      "epoch: 49, batch_id: 5000, loss is: 0.0612\n",
      "epoch: 49, batch_id: 5500, loss is: 0.1203\n",
      "[validation] accuracy/loss: 0.9866/0.0407\n",
      "epoch: 50, batch_id:    0, loss is: 0.0117\n",
      "epoch: 50, batch_id:  500, loss is: 0.0029\n",
      "epoch: 50, batch_id: 1000, loss is: 0.0003\n",
      "epoch: 50, batch_id: 1500, loss is: 0.0430\n",
      "epoch: 50, batch_id: 2000, loss is: 0.0121\n",
      "epoch: 50, batch_id: 2500, loss is: 0.0207\n",
      "epoch: 50, batch_id: 3000, loss is: 0.0112\n",
      "epoch: 50, batch_id: 3500, loss is: 0.1146\n",
      "epoch: 50, batch_id: 4000, loss is: 0.0030\n",
      "epoch: 50, batch_id: 4500, loss is: 0.0014\n",
      "epoch: 50, batch_id: 5000, loss is: 0.0105\n",
      "epoch: 50, batch_id: 5500, loss is: 0.0160\n",
      "[validation] accuracy/loss: 0.9865/0.0406\n",
      "epoch: 51, batch_id:    0, loss is: 0.0013\n",
      "epoch: 51, batch_id:  500, loss is: 0.0040\n",
      "epoch: 51, batch_id: 1000, loss is: 0.0012\n",
      "epoch: 51, batch_id: 1500, loss is: 0.0530\n",
      "epoch: 51, batch_id: 2000, loss is: 0.0021\n",
      "epoch: 51, batch_id: 2500, loss is: 0.0001\n",
      "epoch: 51, batch_id: 3000, loss is: 0.0007\n",
      "epoch: 51, batch_id: 3500, loss is: 0.0014\n",
      "epoch: 51, batch_id: 4000, loss is: 0.0024\n",
      "epoch: 51, batch_id: 4500, loss is: 0.0083\n",
      "epoch: 51, batch_id: 5000, loss is: 0.0042\n",
      "epoch: 51, batch_id: 5500, loss is: 0.0367\n",
      "[validation] accuracy/loss: 0.9859/0.0409\n",
      "epoch: 52, batch_id:    0, loss is: 0.0149\n",
      "epoch: 52, batch_id:  500, loss is: 0.0004\n",
      "epoch: 52, batch_id: 1000, loss is: 0.0011\n",
      "epoch: 52, batch_id: 1500, loss is: 0.0103\n",
      "epoch: 52, batch_id: 2000, loss is: 0.0010\n",
      "epoch: 52, batch_id: 2500, loss is: 0.0021\n",
      "epoch: 52, batch_id: 3000, loss is: 0.0226\n",
      "epoch: 52, batch_id: 3500, loss is: 0.1184\n",
      "epoch: 52, batch_id: 4000, loss is: 0.0054\n",
      "epoch: 52, batch_id: 4500, loss is: 0.0061\n",
      "epoch: 52, batch_id: 5000, loss is: 0.0208\n",
      "epoch: 52, batch_id: 5500, loss is: 0.0019\n",
      "[validation] accuracy/loss: 0.9871/0.0411\n",
      "epoch: 53, batch_id:    0, loss is: 0.0017\n",
      "epoch: 53, batch_id:  500, loss is: 0.0028\n",
      "epoch: 53, batch_id: 1000, loss is: 0.0087\n",
      "epoch: 53, batch_id: 1500, loss is: 0.0056\n",
      "epoch: 53, batch_id: 2000, loss is: 0.0025\n",
      "epoch: 53, batch_id: 2500, loss is: 0.0029\n",
      "epoch: 53, batch_id: 3000, loss is: 0.0051\n",
      "epoch: 53, batch_id: 3500, loss is: 0.0027\n",
      "epoch: 53, batch_id: 4000, loss is: 0.1196\n",
      "epoch: 53, batch_id: 4500, loss is: 0.1467\n",
      "epoch: 53, batch_id: 5000, loss is: 0.0093\n",
      "epoch: 53, batch_id: 5500, loss is: 0.0003\n",
      "[validation] accuracy/loss: 0.9865/0.0412\n",
      "epoch: 54, batch_id:    0, loss is: 0.0056\n",
      "epoch: 54, batch_id:  500, loss is: 0.0156\n",
      "epoch: 54, batch_id: 1000, loss is: 0.0501\n",
      "epoch: 54, batch_id: 1500, loss is: 0.0145\n",
      "epoch: 54, batch_id: 2000, loss is: 0.0044\n",
      "epoch: 54, batch_id: 2500, loss is: 0.0035\n",
      "epoch: 54, batch_id: 3000, loss is: 0.0009\n",
      "epoch: 54, batch_id: 3500, loss is: 0.0021\n",
      "epoch: 54, batch_id: 4000, loss is: 0.0080\n",
      "epoch: 54, batch_id: 4500, loss is: 0.0640\n",
      "epoch: 54, batch_id: 5000, loss is: 0.0283\n",
      "epoch: 54, batch_id: 5500, loss is: 0.0900\n",
      "[validation] accuracy/loss: 0.9868/0.0401\n",
      "epoch: 55, batch_id:    0, loss is: 0.0156\n",
      "epoch: 55, batch_id:  500, loss is: 0.0123\n",
      "epoch: 55, batch_id: 1000, loss is: 0.0048\n",
      "epoch: 55, batch_id: 1500, loss is: 0.0109\n",
      "epoch: 55, batch_id: 2000, loss is: 0.0693\n",
      "epoch: 55, batch_id: 2500, loss is: 0.0002\n",
      "epoch: 55, batch_id: 3000, loss is: 0.0123\n",
      "epoch: 55, batch_id: 3500, loss is: 0.0020\n",
      "epoch: 55, batch_id: 4000, loss is: 0.0016\n",
      "epoch: 55, batch_id: 4500, loss is: 0.0013\n",
      "epoch: 55, batch_id: 5000, loss is: 0.0174\n",
      "epoch: 55, batch_id: 5500, loss is: 0.0012\n",
      "[validation] accuracy/loss: 0.9863/0.0415\n",
      "epoch: 56, batch_id:    0, loss is: 0.0008\n",
      "epoch: 56, batch_id:  500, loss is: 0.0008\n",
      "epoch: 56, batch_id: 1000, loss is: 0.0026\n",
      "epoch: 56, batch_id: 1500, loss is: 0.0003\n",
      "epoch: 56, batch_id: 2000, loss is: 0.0006\n",
      "epoch: 56, batch_id: 2500, loss is: 0.0119\n",
      "epoch: 56, batch_id: 3000, loss is: 0.0201\n",
      "epoch: 56, batch_id: 3500, loss is: 0.0187\n",
      "epoch: 56, batch_id: 4000, loss is: 0.0008\n",
      "epoch: 56, batch_id: 4500, loss is: 0.0093\n",
      "epoch: 56, batch_id: 5000, loss is: 0.0196\n",
      "epoch: 56, batch_id: 5500, loss is: 0.0073\n",
      "[validation] accuracy/loss: 0.9862/0.0419\n",
      "epoch: 57, batch_id:    0, loss is: 0.0026\n",
      "epoch: 57, batch_id:  500, loss is: 0.0771\n",
      "epoch: 57, batch_id: 1000, loss is: 0.0019\n",
      "epoch: 57, batch_id: 1500, loss is: 0.0268\n",
      "epoch: 57, batch_id: 2000, loss is: 0.0028\n",
      "epoch: 57, batch_id: 2500, loss is: 0.0057\n",
      "epoch: 57, batch_id: 3000, loss is: 0.0066\n",
      "epoch: 57, batch_id: 3500, loss is: 0.0008\n",
      "epoch: 57, batch_id: 4000, loss is: 0.0014\n",
      "epoch: 57, batch_id: 4500, loss is: 0.0101\n",
      "epoch: 57, batch_id: 5000, loss is: 0.0030\n",
      "epoch: 57, batch_id: 5500, loss is: 0.0070\n",
      "[validation] accuracy/loss: 0.9868/0.0401\n",
      "epoch: 58, batch_id:    0, loss is: 0.0009\n",
      "epoch: 58, batch_id:  500, loss is: 0.0753\n",
      "epoch: 58, batch_id: 1000, loss is: 0.0039\n",
      "epoch: 58, batch_id: 1500, loss is: 0.0059\n",
      "epoch: 58, batch_id: 2000, loss is: 0.0033\n",
      "epoch: 58, batch_id: 2500, loss is: 0.0021\n",
      "epoch: 58, batch_id: 3000, loss is: 0.0028\n",
      "epoch: 58, batch_id: 3500, loss is: 0.0227\n",
      "epoch: 58, batch_id: 4000, loss is: 0.0029\n",
      "epoch: 58, batch_id: 4500, loss is: 0.0665\n",
      "epoch: 58, batch_id: 5000, loss is: 0.0183\n",
      "epoch: 58, batch_id: 5500, loss is: 0.0120\n",
      "[validation] accuracy/loss: 0.9861/0.0429\n",
      "epoch: 59, batch_id:    0, loss is: 0.0010\n",
      "epoch: 59, batch_id:  500, loss is: 0.0159\n",
      "epoch: 59, batch_id: 1000, loss is: 0.0124\n",
      "epoch: 59, batch_id: 1500, loss is: 0.0033\n",
      "epoch: 59, batch_id: 2000, loss is: 0.0297\n",
      "epoch: 59, batch_id: 2500, loss is: 0.0039\n",
      "epoch: 59, batch_id: 3000, loss is: 0.0212\n",
      "epoch: 59, batch_id: 3500, loss is: 0.0006\n",
      "epoch: 59, batch_id: 4000, loss is: 0.0083\n",
      "epoch: 59, batch_id: 4500, loss is: 0.0565\n",
      "epoch: 59, batch_id: 5000, loss is: 0.0006\n",
      "epoch: 59, batch_id: 5500, loss is: 0.0010\n",
      "[validation] accuracy/loss: 0.9867/0.0391\n",
      "epoch: 60, batch_id:    0, loss is: 0.0004\n",
      "epoch: 60, batch_id:  500, loss is: 0.0457\n",
      "epoch: 60, batch_id: 1000, loss is: 0.0007\n",
      "epoch: 60, batch_id: 1500, loss is: 0.0005\n",
      "epoch: 60, batch_id: 2000, loss is: 0.0062\n",
      "epoch: 60, batch_id: 2500, loss is: 0.0287\n",
      "epoch: 60, batch_id: 3000, loss is: 0.0036\n",
      "epoch: 60, batch_id: 3500, loss is: 0.1185\n",
      "epoch: 60, batch_id: 4000, loss is: 0.0043\n",
      "epoch: 60, batch_id: 4500, loss is: 0.0060\n",
      "epoch: 60, batch_id: 5000, loss is: 0.0044\n",
      "epoch: 60, batch_id: 5500, loss is: 0.1935\n",
      "[validation] accuracy/loss: 0.9868/0.0409\n",
      "epoch: 61, batch_id:    0, loss is: 0.0019\n",
      "epoch: 61, batch_id:  500, loss is: 0.0011\n",
      "epoch: 61, batch_id: 1000, loss is: 0.0023\n",
      "epoch: 61, batch_id: 1500, loss is: 0.0010\n",
      "epoch: 61, batch_id: 2000, loss is: 0.0059\n",
      "epoch: 61, batch_id: 2500, loss is: 0.0016\n",
      "epoch: 61, batch_id: 3000, loss is: 0.0473\n",
      "epoch: 61, batch_id: 3500, loss is: 0.0015\n",
      "epoch: 61, batch_id: 4000, loss is: 0.0113\n",
      "epoch: 61, batch_id: 4500, loss is: 0.0019\n",
      "epoch: 61, batch_id: 5000, loss is: 0.0005\n",
      "epoch: 61, batch_id: 5500, loss is: 0.0004\n",
      "[validation] accuracy/loss: 0.9875/0.0389\n",
      "epoch: 62, batch_id:    0, loss is: 0.0055\n",
      "epoch: 62, batch_id:  500, loss is: 0.0125\n",
      "epoch: 62, batch_id: 1000, loss is: 0.0003\n",
      "epoch: 62, batch_id: 1500, loss is: 0.0035\n",
      "epoch: 62, batch_id: 2000, loss is: 0.0014\n",
      "epoch: 62, batch_id: 2500, loss is: 0.0055\n",
      "epoch: 62, batch_id: 3000, loss is: 0.0003\n",
      "epoch: 62, batch_id: 3500, loss is: 0.0396\n",
      "epoch: 62, batch_id: 4000, loss is: 0.0072\n",
      "epoch: 62, batch_id: 4500, loss is: 0.0036\n",
      "epoch: 62, batch_id: 5000, loss is: 0.0159\n",
      "epoch: 62, batch_id: 5500, loss is: 0.0027\n",
      "[validation] accuracy/loss: 0.9871/0.0391\n",
      "epoch: 63, batch_id:    0, loss is: 0.0108\n",
      "epoch: 63, batch_id:  500, loss is: 0.1715\n",
      "epoch: 63, batch_id: 1000, loss is: 0.0424\n",
      "epoch: 63, batch_id: 1500, loss is: 0.0026\n",
      "epoch: 63, batch_id: 2000, loss is: 0.0090\n",
      "epoch: 63, batch_id: 2500, loss is: 0.0025\n",
      "epoch: 63, batch_id: 3000, loss is: 0.0003\n",
      "epoch: 63, batch_id: 3500, loss is: 0.0023\n",
      "epoch: 63, batch_id: 4000, loss is: 0.0244\n",
      "epoch: 63, batch_id: 4500, loss is: 0.0312\n",
      "epoch: 63, batch_id: 5000, loss is: 0.0009\n",
      "epoch: 63, batch_id: 5500, loss is: 0.3390\n",
      "[validation] accuracy/loss: 0.9870/0.0396\n",
      "epoch: 64, batch_id:    0, loss is: 0.0002\n",
      "epoch: 64, batch_id:  500, loss is: 0.0048\n",
      "epoch: 64, batch_id: 1000, loss is: 0.0012\n",
      "epoch: 64, batch_id: 1500, loss is: 0.0068\n",
      "epoch: 64, batch_id: 2000, loss is: 0.0420\n",
      "epoch: 64, batch_id: 2500, loss is: 0.0003\n",
      "epoch: 64, batch_id: 3000, loss is: 0.0019\n",
      "epoch: 64, batch_id: 3500, loss is: 0.0017\n",
      "epoch: 64, batch_id: 4000, loss is: 0.0019\n",
      "epoch: 64, batch_id: 4500, loss is: 0.0232\n",
      "epoch: 64, batch_id: 5000, loss is: 0.0011\n",
      "epoch: 64, batch_id: 5500, loss is: 0.0018\n",
      "[validation] accuracy/loss: 0.9869/0.0398\n",
      "epoch: 65, batch_id:    0, loss is: 0.0003\n",
      "epoch: 65, batch_id:  500, loss is: 0.0010\n",
      "epoch: 65, batch_id: 1000, loss is: 0.0240\n",
      "epoch: 65, batch_id: 1500, loss is: 0.0011\n",
      "epoch: 65, batch_id: 2000, loss is: 0.0017\n",
      "epoch: 65, batch_id: 2500, loss is: 0.0007\n",
      "epoch: 65, batch_id: 3000, loss is: 0.0053\n",
      "epoch: 65, batch_id: 3500, loss is: 0.0213\n",
      "epoch: 65, batch_id: 4000, loss is: 0.0015\n",
      "epoch: 65, batch_id: 4500, loss is: 0.0190\n",
      "epoch: 65, batch_id: 5000, loss is: 0.0002\n",
      "epoch: 65, batch_id: 5500, loss is: 0.0002\n",
      "[validation] accuracy/loss: 0.9872/0.0394\n",
      "epoch: 66, batch_id:    0, loss is: 0.0154\n",
      "epoch: 66, batch_id:  500, loss is: 0.0002\n",
      "epoch: 66, batch_id: 1000, loss is: 0.0002\n",
      "epoch: 66, batch_id: 1500, loss is: 0.0035\n",
      "epoch: 66, batch_id: 2000, loss is: 0.0004\n",
      "epoch: 66, batch_id: 2500, loss is: 0.1163\n",
      "epoch: 66, batch_id: 3000, loss is: 0.0004\n",
      "epoch: 66, batch_id: 3500, loss is: 0.0073\n",
      "epoch: 66, batch_id: 4000, loss is: 0.0083\n",
      "epoch: 66, batch_id: 4500, loss is: 0.0015\n",
      "epoch: 66, batch_id: 5000, loss is: 0.0004\n",
      "epoch: 66, batch_id: 5500, loss is: 0.1240\n",
      "[validation] accuracy/loss: 0.9873/0.0400\n",
      "epoch: 67, batch_id:    0, loss is: 0.0051\n",
      "epoch: 67, batch_id:  500, loss is: 0.0002\n",
      "epoch: 67, batch_id: 1000, loss is: 0.0108\n",
      "epoch: 67, batch_id: 1500, loss is: 0.0052\n",
      "epoch: 67, batch_id: 2000, loss is: 0.0011\n",
      "epoch: 67, batch_id: 2500, loss is: 0.0002\n",
      "epoch: 67, batch_id: 3000, loss is: 0.0011\n",
      "epoch: 67, batch_id: 3500, loss is: 0.0006\n",
      "epoch: 67, batch_id: 4000, loss is: 0.0027\n",
      "epoch: 67, batch_id: 4500, loss is: 0.0556\n",
      "epoch: 67, batch_id: 5000, loss is: 0.0007\n",
      "epoch: 67, batch_id: 5500, loss is: 0.0055\n",
      "[validation] accuracy/loss: 0.9873/0.0384\n",
      "epoch: 68, batch_id:    0, loss is: 0.0211\n",
      "epoch: 68, batch_id:  500, loss is: 0.0019\n",
      "epoch: 68, batch_id: 1000, loss is: 0.0003\n",
      "epoch: 68, batch_id: 1500, loss is: 0.0006\n",
      "epoch: 68, batch_id: 2000, loss is: 0.0013\n",
      "epoch: 68, batch_id: 2500, loss is: 0.0001\n",
      "epoch: 68, batch_id: 3000, loss is: 0.0005\n",
      "epoch: 68, batch_id: 3500, loss is: 0.0179\n",
      "epoch: 68, batch_id: 4000, loss is: 0.0013\n",
      "epoch: 68, batch_id: 4500, loss is: 0.0013\n",
      "epoch: 68, batch_id: 5000, loss is: 0.0210\n",
      "epoch: 68, batch_id: 5500, loss is: 0.0025\n",
      "[validation] accuracy/loss: 0.9867/0.0394\n",
      "epoch: 69, batch_id:    0, loss is: 0.0007\n",
      "epoch: 69, batch_id:  500, loss is: 0.0105\n",
      "epoch: 69, batch_id: 1000, loss is: 0.0003\n",
      "epoch: 69, batch_id: 1500, loss is: 0.0103\n",
      "epoch: 69, batch_id: 2000, loss is: 0.0004\n",
      "epoch: 69, batch_id: 2500, loss is: 0.0012\n",
      "epoch: 69, batch_id: 3000, loss is: 0.0011\n",
      "epoch: 69, batch_id: 3500, loss is: 0.0219\n",
      "epoch: 69, batch_id: 4000, loss is: 0.0054\n",
      "epoch: 69, batch_id: 4500, loss is: 0.0248\n",
      "epoch: 69, batch_id: 5000, loss is: 0.0104\n",
      "epoch: 69, batch_id: 5500, loss is: 0.0216\n",
      "[validation] accuracy/loss: 0.9867/0.0394\n",
      "epoch: 70, batch_id:    0, loss is: 0.0004\n",
      "epoch: 70, batch_id:  500, loss is: 0.0138\n",
      "epoch: 70, batch_id: 1000, loss is: 0.0010\n",
      "epoch: 70, batch_id: 1500, loss is: 0.0005\n",
      "epoch: 70, batch_id: 2000, loss is: 0.0019\n",
      "epoch: 70, batch_id: 2500, loss is: 0.0015\n",
      "epoch: 70, batch_id: 3000, loss is: 0.0011\n",
      "epoch: 70, batch_id: 3500, loss is: 0.0043\n",
      "epoch: 70, batch_id: 4000, loss is: 0.0577\n",
      "epoch: 70, batch_id: 4500, loss is: 0.0037\n",
      "epoch: 70, batch_id: 5000, loss is: 0.0963\n",
      "epoch: 70, batch_id: 5500, loss is: 0.0042\n",
      "[validation] accuracy/loss: 0.9865/0.0392\n",
      "epoch: 71, batch_id:    0, loss is: 0.0011\n",
      "epoch: 71, batch_id:  500, loss is: 0.0496\n",
      "epoch: 71, batch_id: 1000, loss is: 0.0127\n",
      "epoch: 71, batch_id: 1500, loss is: 0.0012\n",
      "epoch: 71, batch_id: 2000, loss is: 0.0541\n",
      "epoch: 71, batch_id: 2500, loss is: 0.0020\n",
      "epoch: 71, batch_id: 3000, loss is: 0.0005\n",
      "epoch: 71, batch_id: 3500, loss is: 0.0010\n",
      "epoch: 71, batch_id: 4000, loss is: 0.0001\n",
      "epoch: 71, batch_id: 4500, loss is: 0.0011\n",
      "epoch: 71, batch_id: 5000, loss is: 0.0008\n",
      "epoch: 71, batch_id: 5500, loss is: 0.0034\n",
      "[validation] accuracy/loss: 0.9870/0.0389\n",
      "epoch: 72, batch_id:    0, loss is: 0.0054\n",
      "epoch: 72, batch_id:  500, loss is: 0.0128\n",
      "epoch: 72, batch_id: 1000, loss is: 0.0238\n",
      "epoch: 72, batch_id: 1500, loss is: 0.0421\n",
      "epoch: 72, batch_id: 2000, loss is: 0.0003\n",
      "epoch: 72, batch_id: 2500, loss is: 0.0006\n",
      "epoch: 72, batch_id: 3000, loss is: 0.1075\n",
      "epoch: 72, batch_id: 3500, loss is: 0.0050\n",
      "epoch: 72, batch_id: 4000, loss is: 0.0058\n",
      "epoch: 72, batch_id: 4500, loss is: 0.0011\n",
      "epoch: 72, batch_id: 5000, loss is: 0.0462\n",
      "epoch: 72, batch_id: 5500, loss is: 0.0004\n",
      "[validation] accuracy/loss: 0.9878/0.0388\n",
      "epoch: 73, batch_id:    0, loss is: 0.0009\n",
      "epoch: 73, batch_id:  500, loss is: 0.0021\n",
      "epoch: 73, batch_id: 1000, loss is: 0.0022\n",
      "epoch: 73, batch_id: 1500, loss is: 0.0014\n",
      "epoch: 73, batch_id: 2000, loss is: 0.0229\n",
      "epoch: 73, batch_id: 2500, loss is: 0.0003\n",
      "epoch: 73, batch_id: 3000, loss is: 0.0079\n",
      "epoch: 73, batch_id: 3500, loss is: 0.0003\n",
      "epoch: 73, batch_id: 4000, loss is: 0.0005\n",
      "epoch: 73, batch_id: 4500, loss is: 0.0016\n",
      "epoch: 73, batch_id: 5000, loss is: 0.0032\n",
      "epoch: 73, batch_id: 5500, loss is: 0.0003\n",
      "[validation] accuracy/loss: 0.9874/0.0385\n",
      "epoch: 74, batch_id:    0, loss is: 0.0069\n",
      "epoch: 74, batch_id:  500, loss is: 0.0001\n",
      "epoch: 74, batch_id: 1000, loss is: 0.0008\n",
      "epoch: 74, batch_id: 1500, loss is: 0.0738\n",
      "epoch: 74, batch_id: 2000, loss is: 0.0003\n",
      "epoch: 74, batch_id: 2500, loss is: 0.0022\n",
      "epoch: 74, batch_id: 3000, loss is: 0.0019\n",
      "epoch: 74, batch_id: 3500, loss is: 0.0013\n",
      "epoch: 74, batch_id: 4000, loss is: 0.0001\n",
      "epoch: 74, batch_id: 4500, loss is: 0.0009\n",
      "epoch: 74, batch_id: 5000, loss is: 0.0057\n",
      "epoch: 74, batch_id: 5500, loss is: 0.2883\n",
      "[validation] accuracy/loss: 0.9866/0.0411\n",
      "epoch: 75, batch_id:    0, loss is: 0.0007\n",
      "epoch: 75, batch_id:  500, loss is: 0.0080\n",
      "epoch: 75, batch_id: 1000, loss is: 0.0047\n",
      "epoch: 75, batch_id: 1500, loss is: 0.0004\n",
      "epoch: 75, batch_id: 2000, loss is: 0.0023\n",
      "epoch: 75, batch_id: 2500, loss is: 0.0174\n",
      "epoch: 75, batch_id: 3000, loss is: 0.0472\n",
      "epoch: 75, batch_id: 3500, loss is: 0.0085\n",
      "epoch: 75, batch_id: 4000, loss is: 0.0017\n",
      "epoch: 75, batch_id: 4500, loss is: 0.0004\n",
      "epoch: 75, batch_id: 5000, loss is: 0.0052\n",
      "epoch: 75, batch_id: 5500, loss is: 0.0059\n",
      "[validation] accuracy/loss: 0.9878/0.0390\n",
      "epoch: 76, batch_id:    0, loss is: 0.0059\n",
      "epoch: 76, batch_id:  500, loss is: 0.0037\n",
      "epoch: 76, batch_id: 1000, loss is: 0.0065\n",
      "epoch: 76, batch_id: 1500, loss is: 0.0004\n",
      "epoch: 76, batch_id: 2000, loss is: 0.1286\n",
      "epoch: 76, batch_id: 2500, loss is: 0.0005\n",
      "epoch: 76, batch_id: 3000, loss is: 0.0009\n",
      "epoch: 76, batch_id: 3500, loss is: 0.0041\n",
      "epoch: 76, batch_id: 4000, loss is: 0.0085\n",
      "epoch: 76, batch_id: 4500, loss is: 0.0010\n",
      "epoch: 76, batch_id: 5000, loss is: 0.0049\n",
      "epoch: 76, batch_id: 5500, loss is: 0.1775\n",
      "[validation] accuracy/loss: 0.9872/0.0390\n",
      "epoch: 77, batch_id:    0, loss is: 0.0025\n",
      "epoch: 77, batch_id:  500, loss is: 0.0039\n",
      "epoch: 77, batch_id: 1000, loss is: 0.0053\n",
      "epoch: 77, batch_id: 1500, loss is: 0.0399\n",
      "epoch: 77, batch_id: 2000, loss is: 0.0037\n",
      "epoch: 77, batch_id: 2500, loss is: 0.0076\n",
      "epoch: 77, batch_id: 3000, loss is: 0.0006\n",
      "epoch: 77, batch_id: 3500, loss is: 0.0003\n",
      "epoch: 77, batch_id: 4000, loss is: 0.0431\n",
      "epoch: 77, batch_id: 4500, loss is: 0.0001\n",
      "epoch: 77, batch_id: 5000, loss is: 0.0226\n",
      "epoch: 77, batch_id: 5500, loss is: 0.0958\n",
      "[validation] accuracy/loss: 0.9870/0.0390\n",
      "epoch: 78, batch_id:    0, loss is: 0.0007\n",
      "epoch: 78, batch_id:  500, loss is: 0.0004\n",
      "epoch: 78, batch_id: 1000, loss is: 0.0037\n",
      "epoch: 78, batch_id: 1500, loss is: 0.0093\n",
      "epoch: 78, batch_id: 2000, loss is: 0.0029\n",
      "epoch: 78, batch_id: 2500, loss is: 0.0077\n",
      "epoch: 78, batch_id: 3000, loss is: 0.0007\n",
      "epoch: 78, batch_id: 3500, loss is: 0.0002\n",
      "epoch: 78, batch_id: 4000, loss is: 0.0022\n",
      "epoch: 78, batch_id: 4500, loss is: 0.0025\n",
      "epoch: 78, batch_id: 5000, loss is: 0.0023\n",
      "epoch: 78, batch_id: 5500, loss is: 0.0054\n",
      "[validation] accuracy/loss: 0.9876/0.0388\n",
      "epoch: 79, batch_id:    0, loss is: 0.0253\n",
      "epoch: 79, batch_id:  500, loss is: 0.0032\n",
      "epoch: 79, batch_id: 1000, loss is: 0.0003\n",
      "epoch: 79, batch_id: 1500, loss is: 0.0148\n",
      "epoch: 79, batch_id: 2000, loss is: 0.0053\n",
      "epoch: 79, batch_id: 2500, loss is: 0.0268\n",
      "epoch: 79, batch_id: 3000, loss is: 0.0016\n",
      "epoch: 79, batch_id: 3500, loss is: 0.0011\n",
      "epoch: 79, batch_id: 4000, loss is: 0.0017\n",
      "epoch: 79, batch_id: 4500, loss is: 0.0008\n",
      "epoch: 79, batch_id: 5000, loss is: 0.0004\n",
      "epoch: 79, batch_id: 5500, loss is: 0.0099\n",
      "[validation] accuracy/loss: 0.9873/0.0390\n",
      "epoch: 80, batch_id:    0, loss is: 0.0014\n",
      "epoch: 80, batch_id:  500, loss is: 0.0034\n",
      "epoch: 80, batch_id: 1000, loss is: 0.0001\n",
      "epoch: 80, batch_id: 1500, loss is: 0.0028\n",
      "epoch: 80, batch_id: 2000, loss is: 0.0075\n",
      "epoch: 80, batch_id: 2500, loss is: 0.0004\n",
      "epoch: 80, batch_id: 3000, loss is: 0.1172\n",
      "epoch: 80, batch_id: 3500, loss is: 0.0004\n",
      "epoch: 80, batch_id: 4000, loss is: 0.0218\n",
      "epoch: 80, batch_id: 4500, loss is: 0.0225\n",
      "epoch: 80, batch_id: 5000, loss is: 0.0005\n",
      "epoch: 80, batch_id: 5500, loss is: 0.0357\n",
      "[validation] accuracy/loss: 0.9872/0.0378\n",
      "epoch: 81, batch_id:    0, loss is: 0.0003\n",
      "epoch: 81, batch_id:  500, loss is: 0.0389\n",
      "epoch: 81, batch_id: 1000, loss is: 0.2319\n",
      "epoch: 81, batch_id: 1500, loss is: 0.0145\n",
      "epoch: 81, batch_id: 2000, loss is: 0.0003\n",
      "epoch: 81, batch_id: 2500, loss is: 0.0025\n",
      "epoch: 81, batch_id: 3000, loss is: 0.0391\n",
      "epoch: 81, batch_id: 3500, loss is: 0.0096\n",
      "epoch: 81, batch_id: 4000, loss is: 0.6581\n",
      "epoch: 81, batch_id: 4500, loss is: 0.0003\n",
      "epoch: 81, batch_id: 5000, loss is: 0.0116\n",
      "epoch: 81, batch_id: 5500, loss is: 0.0326\n",
      "[validation] accuracy/loss: 0.9875/0.0383\n",
      "epoch: 82, batch_id:    0, loss is: 0.0013\n",
      "epoch: 82, batch_id:  500, loss is: 0.0035\n",
      "epoch: 82, batch_id: 1000, loss is: 0.0148\n",
      "epoch: 82, batch_id: 1500, loss is: 0.0805\n",
      "epoch: 82, batch_id: 2000, loss is: 0.0002\n",
      "epoch: 82, batch_id: 2500, loss is: 0.0003\n",
      "epoch: 82, batch_id: 3000, loss is: 0.0034\n",
      "epoch: 82, batch_id: 3500, loss is: 0.0003\n",
      "epoch: 82, batch_id: 4000, loss is: 0.0010\n",
      "epoch: 82, batch_id: 4500, loss is: 0.0002\n",
      "epoch: 82, batch_id: 5000, loss is: 0.0003\n",
      "epoch: 82, batch_id: 5500, loss is: 0.0025\n",
      "[validation] accuracy/loss: 0.9868/0.0395\n",
      "epoch: 83, batch_id:    0, loss is: 0.0004\n",
      "epoch: 83, batch_id:  500, loss is: 0.0197\n",
      "epoch: 83, batch_id: 1000, loss is: 0.0275\n",
      "epoch: 83, batch_id: 1500, loss is: 0.0159\n",
      "epoch: 83, batch_id: 2000, loss is: 0.0011\n",
      "epoch: 83, batch_id: 2500, loss is: 0.0030\n",
      "epoch: 83, batch_id: 3000, loss is: 0.0018\n",
      "epoch: 83, batch_id: 3500, loss is: 0.0008\n",
      "epoch: 83, batch_id: 4000, loss is: 0.0102\n",
      "epoch: 83, batch_id: 4500, loss is: 0.0010\n",
      "epoch: 83, batch_id: 5000, loss is: 0.0105\n",
      "epoch: 83, batch_id: 5500, loss is: 0.0016\n",
      "[validation] accuracy/loss: 0.9875/0.0383\n",
      "epoch: 84, batch_id:    0, loss is: 0.0001\n",
      "epoch: 84, batch_id:  500, loss is: 0.0009\n",
      "epoch: 84, batch_id: 1000, loss is: 0.0021\n",
      "epoch: 84, batch_id: 1500, loss is: 0.0007\n",
      "epoch: 84, batch_id: 2000, loss is: 0.0003\n",
      "epoch: 84, batch_id: 2500, loss is: 0.0022\n",
      "epoch: 84, batch_id: 3000, loss is: 0.0172\n",
      "epoch: 84, batch_id: 3500, loss is: 0.0003\n",
      "epoch: 84, batch_id: 4000, loss is: 0.0056\n",
      "epoch: 84, batch_id: 4500, loss is: 0.0163\n",
      "epoch: 84, batch_id: 5000, loss is: 0.0011\n",
      "epoch: 84, batch_id: 5500, loss is: 0.0013\n",
      "[validation] accuracy/loss: 0.9877/0.0384\n",
      "epoch: 85, batch_id:    0, loss is: 0.0018\n",
      "epoch: 85, batch_id:  500, loss is: 0.0002\n",
      "epoch: 85, batch_id: 1000, loss is: 0.0014\n",
      "epoch: 85, batch_id: 1500, loss is: 0.0064\n",
      "epoch: 85, batch_id: 2000, loss is: 0.0006\n",
      "epoch: 85, batch_id: 2500, loss is: 0.0027\n",
      "epoch: 85, batch_id: 3000, loss is: 0.0431\n",
      "epoch: 85, batch_id: 3500, loss is: 0.0007\n",
      "epoch: 85, batch_id: 4000, loss is: 0.0009\n",
      "epoch: 85, batch_id: 4500, loss is: 0.0373\n",
      "epoch: 85, batch_id: 5000, loss is: 0.0017\n",
      "epoch: 85, batch_id: 5500, loss is: 0.0266\n",
      "[validation] accuracy/loss: 0.9875/0.0383\n",
      "epoch: 86, batch_id:    0, loss is: 0.0007\n",
      "epoch: 86, batch_id:  500, loss is: 0.0014\n",
      "epoch: 86, batch_id: 1000, loss is: 0.0012\n",
      "epoch: 86, batch_id: 1500, loss is: 0.0075\n",
      "epoch: 86, batch_id: 2000, loss is: 0.0028\n",
      "epoch: 86, batch_id: 2500, loss is: 0.0002\n",
      "epoch: 86, batch_id: 3000, loss is: 0.0001\n",
      "epoch: 86, batch_id: 3500, loss is: 0.0085\n",
      "epoch: 86, batch_id: 4000, loss is: 0.0045\n",
      "epoch: 86, batch_id: 4500, loss is: 0.0005\n",
      "epoch: 86, batch_id: 5000, loss is: 0.0005\n",
      "epoch: 86, batch_id: 5500, loss is: 0.0013\n",
      "[validation] accuracy/loss: 0.9871/0.0387\n",
      "epoch: 87, batch_id:    0, loss is: 0.0036\n",
      "epoch: 87, batch_id:  500, loss is: 0.0015\n",
      "epoch: 87, batch_id: 1000, loss is: 0.0002\n",
      "epoch: 87, batch_id: 1500, loss is: 0.0155\n",
      "epoch: 87, batch_id: 2000, loss is: 0.0041\n",
      "epoch: 87, batch_id: 2500, loss is: 0.0282\n",
      "epoch: 87, batch_id: 3000, loss is: 0.0032\n",
      "epoch: 87, batch_id: 3500, loss is: 0.0046\n",
      "epoch: 87, batch_id: 4000, loss is: 0.0008\n",
      "epoch: 87, batch_id: 4500, loss is: 0.0003\n",
      "epoch: 87, batch_id: 5000, loss is: 0.0004\n",
      "epoch: 87, batch_id: 5500, loss is: 0.0436\n",
      "[validation] accuracy/loss: 0.9872/0.0399\n",
      "epoch: 88, batch_id:    0, loss is: 0.0009\n",
      "epoch: 88, batch_id:  500, loss is: 0.0015\n",
      "epoch: 88, batch_id: 1000, loss is: 0.0151\n",
      "epoch: 88, batch_id: 1500, loss is: 0.0011\n",
      "epoch: 88, batch_id: 2000, loss is: 0.0004\n",
      "epoch: 88, batch_id: 2500, loss is: 0.0010\n",
      "epoch: 88, batch_id: 3000, loss is: 0.0005\n",
      "epoch: 88, batch_id: 3500, loss is: 0.0015\n",
      "epoch: 88, batch_id: 4000, loss is: 0.0019\n",
      "epoch: 88, batch_id: 4500, loss is: 0.0049\n",
      "epoch: 88, batch_id: 5000, loss is: 0.0008\n",
      "epoch: 88, batch_id: 5500, loss is: 0.0013\n",
      "[validation] accuracy/loss: 0.9874/0.0395\n",
      "epoch: 89, batch_id:    0, loss is: 0.0099\n",
      "epoch: 89, batch_id:  500, loss is: 0.0019\n",
      "epoch: 89, batch_id: 1000, loss is: 0.0062\n",
      "epoch: 89, batch_id: 1500, loss is: 0.0003\n",
      "epoch: 89, batch_id: 2000, loss is: 0.0032\n",
      "epoch: 89, batch_id: 2500, loss is: 0.0035\n",
      "epoch: 89, batch_id: 3000, loss is: 0.0001\n",
      "epoch: 89, batch_id: 3500, loss is: 0.0003\n",
      "epoch: 89, batch_id: 4000, loss is: 0.1008\n",
      "epoch: 89, batch_id: 4500, loss is: 0.0103\n",
      "epoch: 89, batch_id: 5000, loss is: 0.0530\n",
      "epoch: 89, batch_id: 5500, loss is: 0.0002\n",
      "[validation] accuracy/loss: 0.9884/0.0376\n",
      "epoch: 90, batch_id:    0, loss is: 0.0429\n",
      "epoch: 90, batch_id:  500, loss is: 0.0070\n",
      "epoch: 90, batch_id: 1000, loss is: 0.0005\n",
      "epoch: 90, batch_id: 1500, loss is: 0.1546\n",
      "epoch: 90, batch_id: 2000, loss is: 0.0079\n",
      "epoch: 90, batch_id: 2500, loss is: 0.0082\n",
      "epoch: 90, batch_id: 3000, loss is: 0.0083\n",
      "epoch: 90, batch_id: 3500, loss is: 0.0002\n",
      "epoch: 90, batch_id: 4000, loss is: 0.1648\n",
      "epoch: 90, batch_id: 4500, loss is: 0.0103\n",
      "epoch: 90, batch_id: 5000, loss is: 0.0003\n",
      "epoch: 90, batch_id: 5500, loss is: 0.0166\n",
      "[validation] accuracy/loss: 0.9873/0.0392\n",
      "epoch: 91, batch_id:    0, loss is: 0.0004\n",
      "epoch: 91, batch_id:  500, loss is: 0.0078\n",
      "epoch: 91, batch_id: 1000, loss is: 0.0074\n",
      "epoch: 91, batch_id: 1500, loss is: 0.0015\n",
      "epoch: 91, batch_id: 2000, loss is: 0.0115\n",
      "epoch: 91, batch_id: 2500, loss is: 0.0002\n",
      "epoch: 91, batch_id: 3000, loss is: 0.0017\n",
      "epoch: 91, batch_id: 3500, loss is: 0.0868\n",
      "epoch: 91, batch_id: 4000, loss is: 0.0129\n",
      "epoch: 91, batch_id: 4500, loss is: 0.0017\n",
      "epoch: 91, batch_id: 5000, loss is: 0.0028\n",
      "epoch: 91, batch_id: 5500, loss is: 0.0133\n",
      "[validation] accuracy/loss: 0.9871/0.0396\n",
      "epoch: 92, batch_id:    0, loss is: 0.0391\n",
      "epoch: 92, batch_id:  500, loss is: 0.0016\n",
      "epoch: 92, batch_id: 1000, loss is: 0.0046\n",
      "epoch: 92, batch_id: 1500, loss is: 0.0017\n",
      "epoch: 92, batch_id: 2000, loss is: 0.0101\n",
      "epoch: 92, batch_id: 2500, loss is: 0.0023\n",
      "epoch: 92, batch_id: 3000, loss is: 0.0002\n",
      "epoch: 92, batch_id: 3500, loss is: 0.0046\n",
      "epoch: 92, batch_id: 4000, loss is: 0.0036\n",
      "epoch: 92, batch_id: 4500, loss is: 0.0010\n",
      "epoch: 92, batch_id: 5000, loss is: 0.0005\n",
      "epoch: 92, batch_id: 5500, loss is: 0.0009\n",
      "[validation] accuracy/loss: 0.9880/0.0378\n",
      "epoch: 93, batch_id:    0, loss is: 0.0377\n",
      "epoch: 93, batch_id:  500, loss is: 0.1124\n",
      "epoch: 93, batch_id: 1000, loss is: 0.0082\n",
      "epoch: 93, batch_id: 1500, loss is: 0.0071\n",
      "epoch: 93, batch_id: 2000, loss is: 0.0002\n",
      "epoch: 93, batch_id: 2500, loss is: 0.0014\n",
      "epoch: 93, batch_id: 3000, loss is: 0.0006\n",
      "epoch: 93, batch_id: 3500, loss is: 0.0004\n",
      "epoch: 93, batch_id: 4000, loss is: 0.0004\n",
      "epoch: 93, batch_id: 4500, loss is: 0.0019\n",
      "epoch: 93, batch_id: 5000, loss is: 0.0009\n",
      "epoch: 93, batch_id: 5500, loss is: 0.0002\n",
      "[validation] accuracy/loss: 0.9875/0.0387\n",
      "epoch: 94, batch_id:    0, loss is: 0.0006\n",
      "epoch: 94, batch_id:  500, loss is: 0.0002\n",
      "epoch: 94, batch_id: 1000, loss is: 0.0003\n",
      "epoch: 94, batch_id: 1500, loss is: 0.0093\n",
      "epoch: 94, batch_id: 2000, loss is: 0.0004\n",
      "epoch: 94, batch_id: 2500, loss is: 0.0120\n",
      "epoch: 94, batch_id: 3000, loss is: 0.0004\n",
      "epoch: 94, batch_id: 3500, loss is: 0.0031\n",
      "epoch: 94, batch_id: 4000, loss is: 0.0055\n",
      "epoch: 94, batch_id: 4500, loss is: 0.0021\n",
      "epoch: 94, batch_id: 5000, loss is: 0.0087\n",
      "epoch: 94, batch_id: 5500, loss is: 0.0026\n",
      "[validation] accuracy/loss: 0.9869/0.0396\n",
      "epoch: 95, batch_id:    0, loss is: 0.0024\n",
      "epoch: 95, batch_id:  500, loss is: 0.0005\n",
      "epoch: 95, batch_id: 1000, loss is: 0.0016\n",
      "epoch: 95, batch_id: 1500, loss is: 0.0027\n",
      "epoch: 95, batch_id: 2000, loss is: 0.0006\n",
      "epoch: 95, batch_id: 2500, loss is: 0.0024\n",
      "epoch: 95, batch_id: 3000, loss is: 0.0011\n",
      "epoch: 95, batch_id: 3500, loss is: 0.0012\n",
      "epoch: 95, batch_id: 4000, loss is: 0.0177\n",
      "epoch: 95, batch_id: 4500, loss is: 0.0017\n",
      "epoch: 95, batch_id: 5000, loss is: 0.0029\n",
      "epoch: 95, batch_id: 5500, loss is: 0.0020\n",
      "[validation] accuracy/loss: 0.9878/0.0385\n",
      "epoch: 96, batch_id:    0, loss is: 0.0002\n",
      "epoch: 96, batch_id:  500, loss is: 0.0008\n",
      "epoch: 96, batch_id: 1000, loss is: 0.0099\n",
      "epoch: 96, batch_id: 1500, loss is: 0.0051\n",
      "epoch: 96, batch_id: 2000, loss is: 0.0010\n",
      "epoch: 96, batch_id: 2500, loss is: 0.0024\n",
      "epoch: 96, batch_id: 3000, loss is: 0.0025\n",
      "epoch: 96, batch_id: 3500, loss is: 0.0175\n",
      "epoch: 96, batch_id: 4000, loss is: 0.0024\n",
      "epoch: 96, batch_id: 4500, loss is: 0.0111\n",
      "epoch: 96, batch_id: 5000, loss is: 0.0006\n",
      "epoch: 96, batch_id: 5500, loss is: 0.0002\n",
      "[validation] accuracy/loss: 0.9873/0.0387\n",
      "epoch: 97, batch_id:    0, loss is: 0.0163\n",
      "epoch: 97, batch_id:  500, loss is: 0.0002\n",
      "epoch: 97, batch_id: 1000, loss is: 0.0008\n",
      "epoch: 97, batch_id: 1500, loss is: 0.0002\n",
      "epoch: 97, batch_id: 2000, loss is: 0.0216\n",
      "epoch: 97, batch_id: 2500, loss is: 0.0154\n",
      "epoch: 97, batch_id: 3000, loss is: 0.0011\n",
      "epoch: 97, batch_id: 3500, loss is: 0.0085\n",
      "epoch: 97, batch_id: 4000, loss is: 0.0005\n",
      "epoch: 97, batch_id: 4500, loss is: 0.0005\n",
      "epoch: 97, batch_id: 5000, loss is: 0.0000\n",
      "epoch: 97, batch_id: 5500, loss is: 0.0029\n",
      "[validation] accuracy/loss: 0.9875/0.0398\n",
      "epoch: 98, batch_id:    0, loss is: 0.0005\n",
      "epoch: 98, batch_id:  500, loss is: 0.0154\n",
      "epoch: 98, batch_id: 1000, loss is: 0.0069\n",
      "epoch: 98, batch_id: 1500, loss is: 0.0003\n",
      "epoch: 98, batch_id: 2000, loss is: 0.0003\n",
      "epoch: 98, batch_id: 2500, loss is: 0.0557\n",
      "epoch: 98, batch_id: 3000, loss is: 0.0201\n",
      "epoch: 98, batch_id: 3500, loss is: 0.0003\n",
      "epoch: 98, batch_id: 4000, loss is: 0.0005\n",
      "epoch: 98, batch_id: 4500, loss is: 0.0001\n",
      "epoch: 98, batch_id: 5000, loss is: 0.0008\n",
      "epoch: 98, batch_id: 5500, loss is: 0.0005\n",
      "[validation] accuracy/loss: 0.9875/0.0381\n",
      "epoch: 99, batch_id:    0, loss is: 0.0250\n",
      "epoch: 99, batch_id:  500, loss is: 0.0027\n",
      "epoch: 99, batch_id: 1000, loss is: 0.0019\n",
      "epoch: 99, batch_id: 1500, loss is: 0.0138\n",
      "epoch: 99, batch_id: 2000, loss is: 0.0121\n",
      "epoch: 99, batch_id: 2500, loss is: 0.0002\n",
      "epoch: 99, batch_id: 3000, loss is: 0.0002\n",
      "epoch: 99, batch_id: 3500, loss is: 0.0037\n",
      "epoch: 99, batch_id: 4000, loss is: 0.0003\n",
      "epoch: 99, batch_id: 4500, loss is: 0.0003\n",
      "epoch: 99, batch_id: 5000, loss is: 0.0039\n",
      "epoch: 99, batch_id: 5500, loss is: 0.0134\n",
      "[validation] accuracy/loss: 0.9880/0.0391\n",
      "epoch: 100, batch_id:    0, loss is: 0.0010\n",
      "epoch: 100, batch_id:  500, loss is: 0.0429\n",
      "epoch: 100, batch_id: 1000, loss is: 0.0015\n",
      "epoch: 100, batch_id: 1500, loss is: 0.0027\n",
      "epoch: 100, batch_id: 2000, loss is: 0.0002\n",
      "epoch: 100, batch_id: 2500, loss is: 0.0009\n",
      "epoch: 100, batch_id: 3000, loss is: 0.0033\n",
      "epoch: 100, batch_id: 3500, loss is: 0.0019\n",
      "epoch: 100, batch_id: 4000, loss is: 0.0005\n",
      "epoch: 100, batch_id: 4500, loss is: 0.0043\n",
      "epoch: 100, batch_id: 5000, loss is: 0.0016\n",
      "epoch: 100, batch_id: 5500, loss is: 0.0006\n",
      "[validation] accuracy/loss: 0.9874/0.0388\n",
      "-----------finish training----------\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 实例化模型\n",
    "model = LeNet(num_classes=10) # 10分类\n",
    "# 训练轮数\n",
    "EPOCH_NUM = 100\n",
    "# 设置优化器\n",
    "opt = paddle.optimizer.Momentum(learning_rate=0.001, parameters=model.parameters())\n",
    "# 数据集加载（同时进行10张）\n",
    "train_loader = paddle.io.DataLoader(MNIST(mode='train', transform=ToTensor()), batch_size=10, shuffle=True)\n",
    "valid_loader = paddle.io.DataLoader(MNIST(mode='test', transform=ToTensor()), batch_size=10)\n",
    "# 绘制曲线准备\n",
    "all_train_iter = 0\n",
    "all_train_iters=[]\n",
    "all_train_costs=[]\n",
    "all_train_accs=[]\n",
    "\n",
    "# 训练\n",
    "train(model, opt, train_loader, valid_loader)\n",
    "\n",
    "# 绘制曲线\n",
    "draw_train_process(\"training\",all_train_iters,all_train_costs,all_train_accs,\"trainning cost\",\"trainning acc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请点击[此处](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576)查看本环境基本用法.  <br>\n",
    "Please click [here ](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576) for more detailed instructions. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
